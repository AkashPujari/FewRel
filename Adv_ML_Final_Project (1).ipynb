{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adv ML Final Project.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HkKQnObUmTf",
        "outputId": "5a853e04-d632-413b-a647-b487bbbf85d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FewRel'...\n",
            "remote: Enumerating objects: 577, done.\u001b[K\n",
            "remote: Counting objects: 100% (182/182), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 577 (delta 159), reused 152 (delta 145), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (577/577), 24.67 MiB | 17.92 MiB/s, done.\n",
            "Resolving deltas: 100% (373/373), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/AkashPujari/FewRel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzZB_WgwWGfY",
        "outputId": "a5f2d7ca-298d-4f90-8d71-99e4cd89ded3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf '/content/gdrive/MyDrive/pretrain tar file/pretrain.tar' -C 'FewRel/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVcYRc2cVsW5",
        "outputId": "f05503d7-a8c6-4395-82f1-bf44bad81092"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrain/\n",
            "pretrain/glove/\n",
            "pretrain/glove/glove_mat.npy\n",
            "pretrain/glove/glove_word2id.json\n",
            "pretrain/bert-base-uncased/\n",
            "pretrain/bert-base-uncased/pytorch_model.bin\n",
            "pretrain/bert-base-uncased/config.json\n",
            "pretrain/bert-base-uncased/vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd FewRel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P38YtHPXm2c",
        "outputId": "1175b3e4-e4ab-4d83-f87b-3e1ed801a64b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FewRel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9aYx5IVXtpd",
        "outputId": "1cd81cac-fa52-4b0e-cc85-6d5caf4e3258"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train_demo.py \\\n",
        "    --trainN 5 --N 5 --K 1 --Q 1 \\\n",
        "    --model pair --encoder bert --pair --hidden_size 768 --val_step 1000 \\\n",
        "    --batch_size 4 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytX0dKHXXe0E",
        "outputId": "d74a6c84-0b40-42ad-9712-152e366ee02e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way-1-shot Few-Shot Relation Classification\n",
            "model: pair\n",
            "encoder: bert\n",
            "max_length: 128\n",
            "Downloading: 100% 570/570 [00:00<00:00, 597kB/s]\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 72.8MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 926kB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 26.1kB/s]\n",
            "Start training...\n",
            "Using bert optim\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "step:    1 | loss: 1.776315, accuracy: 20.00%\n",
            "step:    2 | loss: 1.785216, accuracy: 17.50%\n",
            "step:    3 | loss: 1.808072, accuracy: 13.33%\n",
            "step:    4 | loss: 1.807569, accuracy: 12.50%\n",
            "step:    5 | loss: 1.820281, accuracy: 10.00%\n",
            "step:    6 | loss: 1.810731, accuracy: 12.50%\n",
            "step:    7 | loss: 1.818575, accuracy: 12.14%\n",
            "step:    8 | loss: 1.820328, accuracy: 11.88%\n",
            "step:    9 | loss: 1.817500, accuracy: 11.67%\n",
            "step:   10 | loss: 1.815530, accuracy: 12.00%\n",
            "step:   11 | loss: 1.816593, accuracy: 12.27%\n",
            "step:   12 | loss: 1.815852, accuracy: 12.50%\n",
            "step:   13 | loss: 1.819663, accuracy: 11.92%\n",
            "step:   14 | loss: 1.819025, accuracy: 12.86%\n",
            "step:   15 | loss: 1.820707, accuracy: 12.00%\n",
            "step:   16 | loss: 1.817164, accuracy: 13.13%\n",
            "step:   17 | loss: 1.816935, accuracy: 13.24%\n",
            "step:   18 | loss: 1.816793, accuracy: 13.06%\n",
            "step:   19 | loss: 1.815389, accuracy: 13.16%\n",
            "step:   20 | loss: 1.814997, accuracy: 13.00%\n",
            "step:   21 | loss: 1.813383, accuracy: 13.33%\n",
            "step:   22 | loss: 1.811099, accuracy: 13.64%\n",
            "step:   23 | loss: 1.809070, accuracy: 13.91%\n",
            "step:   24 | loss: 1.808729, accuracy: 13.96%\n",
            "step:   25 | loss: 1.806291, accuracy: 14.20%\n",
            "step:   26 | loss: 1.800567, accuracy: 15.19%\n",
            "step:   27 | loss: 1.798833, accuracy: 15.37%\n",
            "step:   28 | loss: 1.795630, accuracy: 15.89%\n",
            "step:   29 | loss: 1.792402, accuracy: 16.38%\n",
            "step:   30 | loss: 1.790291, accuracy: 17.00%\n",
            "step:   31 | loss: 1.789895, accuracy: 17.10%\n",
            "step:   32 | loss: 1.787018, accuracy: 17.81%\n",
            "step:   33 | loss: 1.783327, accuracy: 18.64%\n",
            "step:   34 | loss: 1.778500, accuracy: 19.56%\n",
            "step:   35 | loss: 1.776829, accuracy: 20.00%\n",
            "step:   36 | loss: 1.774759, accuracy: 20.28%\n",
            "step:   37 | loss: 1.771562, accuracy: 20.54%\n",
            "step:   38 | loss: 1.768411, accuracy: 21.05%\n",
            "step:   39 | loss: 1.766096, accuracy: 21.41%\n",
            "step:   40 | loss: 1.761803, accuracy: 21.88%\n",
            "step:   41 | loss: 1.758790, accuracy: 22.44%\n",
            "step:   42 | loss: 1.756802, accuracy: 22.38%\n",
            "step:   43 | loss: 1.753401, accuracy: 23.26%\n",
            "step:   44 | loss: 1.748016, accuracy: 23.98%\n",
            "step:   45 | loss: 1.744905, accuracy: 24.56%\n",
            "step:   46 | loss: 1.740287, accuracy: 24.89%\n",
            "step:   47 | loss: 1.735486, accuracy: 25.74%\n",
            "step:   48 | loss: 1.732787, accuracy: 26.04%\n",
            "step:   49 | loss: 1.728279, accuracy: 26.53%\n",
            "step:   50 | loss: 1.724515, accuracy: 26.70%\n",
            "step:   51 | loss: 1.719139, accuracy: 27.35%\n",
            "step:   52 | loss: 1.714947, accuracy: 27.79%\n",
            "step:   53 | loss: 1.712415, accuracy: 28.02%\n",
            "step:   54 | loss: 1.707284, accuracy: 28.52%\n",
            "step:   55 | loss: 1.700780, accuracy: 29.36%\n",
            "step:   56 | loss: 1.693560, accuracy: 29.91%\n",
            "step:   57 | loss: 1.688039, accuracy: 30.35%\n",
            "step:   58 | loss: 1.685757, accuracy: 30.69%\n",
            "step:   59 | loss: 1.676681, accuracy: 31.61%\n",
            "step:   60 | loss: 1.670807, accuracy: 32.00%\n",
            "step:   61 | loss: 1.663008, accuracy: 32.79%\n",
            "step:   62 | loss: 1.656383, accuracy: 33.31%\n",
            "step:   63 | loss: 1.649459, accuracy: 33.89%\n",
            "step:   64 | loss: 1.642299, accuracy: 34.30%\n",
            "step:   65 | loss: 1.634382, accuracy: 34.77%\n",
            "step:   66 | loss: 1.629431, accuracy: 35.00%\n",
            "step:   67 | loss: 1.622512, accuracy: 35.22%\n",
            "step:   68 | loss: 1.617269, accuracy: 35.51%\n",
            "step:   69 | loss: 1.606928, accuracy: 36.23%\n",
            "step:   70 | loss: 1.600099, accuracy: 36.50%\n",
            "step:   71 | loss: 1.589176, accuracy: 37.11%\n",
            "step:   72 | loss: 1.580096, accuracy: 37.64%\n",
            "step:   73 | loss: 1.576665, accuracy: 37.81%\n",
            "step:   74 | loss: 1.566603, accuracy: 38.24%\n",
            "step:   75 | loss: 1.557955, accuracy: 38.60%\n",
            "step:   76 | loss: 1.548178, accuracy: 39.01%\n",
            "step:   77 | loss: 1.540363, accuracy: 39.42%\n",
            "step:   78 | loss: 1.533145, accuracy: 39.74%\n",
            "step:   79 | loss: 1.526405, accuracy: 40.13%\n",
            "step:   80 | loss: 1.520923, accuracy: 40.38%\n",
            "step:   81 | loss: 1.513745, accuracy: 40.74%\n",
            "step:   82 | loss: 1.508450, accuracy: 41.04%\n",
            "step:   83 | loss: 1.502733, accuracy: 41.39%\n",
            "step:   84 | loss: 1.493604, accuracy: 41.85%\n",
            "step:   85 | loss: 1.489185, accuracy: 42.00%\n",
            "step:   86 | loss: 1.481196, accuracy: 42.33%\n",
            "step:   87 | loss: 1.471778, accuracy: 42.70%\n",
            "step:   88 | loss: 1.464238, accuracy: 42.90%\n",
            "step:   89 | loss: 1.454598, accuracy: 43.31%\n",
            "step:   90 | loss: 1.448311, accuracy: 43.50%\n",
            "step:   91 | loss: 1.440407, accuracy: 43.90%\n",
            "step:   92 | loss: 1.436116, accuracy: 43.97%\n",
            "step:   93 | loss: 1.426462, accuracy: 44.46%\n",
            "step:   94 | loss: 1.420411, accuracy: 44.68%\n",
            "step:   95 | loss: 1.414308, accuracy: 44.84%\n",
            "step:   96 | loss: 1.405112, accuracy: 45.26%\n",
            "step:   97 | loss: 1.400412, accuracy: 45.36%\n",
            "step:   98 | loss: 1.395691, accuracy: 45.56%\n",
            "step:   99 | loss: 1.388684, accuracy: 45.86%\n",
            "step:  100 | loss: 1.384907, accuracy: 45.95%\n",
            "step:  101 | loss: 1.382288, accuracy: 45.94%\n",
            "step:  102 | loss: 1.374673, accuracy: 46.37%\n",
            "step:  103 | loss: 1.370993, accuracy: 46.50%\n",
            "step:  104 | loss: 1.366370, accuracy: 46.73%\n",
            "step:  105 | loss: 1.361640, accuracy: 46.95%\n",
            "step:  106 | loss: 1.354502, accuracy: 47.22%\n",
            "step:  107 | loss: 1.351518, accuracy: 47.34%\n",
            "step:  108 | loss: 1.346571, accuracy: 47.55%\n",
            "step:  109 | loss: 1.340354, accuracy: 47.61%\n",
            "step:  110 | loss: 1.335754, accuracy: 47.77%\n",
            "step:  111 | loss: 1.331439, accuracy: 47.93%\n",
            "step:  112 | loss: 1.327518, accuracy: 48.21%\n",
            "step:  113 | loss: 1.324496, accuracy: 48.41%\n",
            "step:  114 | loss: 1.318093, accuracy: 48.77%\n",
            "step:  115 | loss: 1.315286, accuracy: 48.87%\n",
            "step:  116 | loss: 1.310410, accuracy: 49.09%\n",
            "step:  117 | loss: 1.307668, accuracy: 49.27%\n",
            "step:  118 | loss: 1.302203, accuracy: 49.49%\n",
            "step:  119 | loss: 1.295097, accuracy: 49.83%\n",
            "step:  120 | loss: 1.292660, accuracy: 49.92%\n",
            "step:  121 | loss: 1.286138, accuracy: 50.21%\n",
            "step:  122 | loss: 1.282007, accuracy: 50.41%\n",
            "step:  123 | loss: 1.277798, accuracy: 50.53%\n",
            "step:  124 | loss: 1.274159, accuracy: 50.73%\n",
            "step:  125 | loss: 1.270770, accuracy: 50.92%\n",
            "step:  126 | loss: 1.266656, accuracy: 51.03%\n",
            "step:  127 | loss: 1.262431, accuracy: 51.30%\n",
            "step:  128 | loss: 1.261034, accuracy: 51.29%\n",
            "step:  129 | loss: 1.257987, accuracy: 51.43%\n",
            "step:  130 | loss: 1.255045, accuracy: 51.58%\n",
            "step:  131 | loss: 1.250438, accuracy: 51.79%\n",
            "step:  132 | loss: 1.245727, accuracy: 51.97%\n",
            "step:  133 | loss: 1.239531, accuracy: 52.22%\n",
            "step:  134 | loss: 1.236622, accuracy: 52.31%\n",
            "step:  135 | loss: 1.232586, accuracy: 52.44%\n",
            "step:  136 | loss: 1.227309, accuracy: 52.57%\n",
            "step:  137 | loss: 1.224059, accuracy: 52.74%\n",
            "step:  138 | loss: 1.218953, accuracy: 53.08%\n",
            "step:  139 | loss: 1.216008, accuracy: 53.20%\n",
            "step:  140 | loss: 1.210015, accuracy: 53.50%\n",
            "step:  141 | loss: 1.208143, accuracy: 53.65%\n",
            "step:  142 | loss: 1.204077, accuracy: 53.80%\n",
            "step:  143 | loss: 1.199901, accuracy: 53.95%\n",
            "step:  144 | loss: 1.195576, accuracy: 54.13%\n",
            "step:  145 | loss: 1.191319, accuracy: 54.34%\n",
            "step:  146 | loss: 1.188795, accuracy: 54.42%\n",
            "step:  147 | loss: 1.184515, accuracy: 54.59%\n",
            "step:  148 | loss: 1.181294, accuracy: 54.73%\n",
            "step:  149 | loss: 1.178228, accuracy: 54.87%\n",
            "step:  150 | loss: 1.174099, accuracy: 55.00%\n",
            "step:  151 | loss: 1.171527, accuracy: 55.07%\n",
            "step:  152 | loss: 1.168591, accuracy: 55.20%\n",
            "step:  153 | loss: 1.164447, accuracy: 55.36%\n",
            "step:  154 | loss: 1.161045, accuracy: 55.49%\n",
            "step:  155 | loss: 1.156034, accuracy: 55.71%\n",
            "step:  156 | loss: 1.151606, accuracy: 55.93%\n",
            "step:  157 | loss: 1.148505, accuracy: 56.05%\n",
            "step:  158 | loss: 1.148564, accuracy: 55.98%\n",
            "step:  159 | loss: 1.145961, accuracy: 56.10%\n",
            "step:  160 | loss: 1.142343, accuracy: 56.25%\n",
            "step:  161 | loss: 1.138315, accuracy: 56.40%\n",
            "step:  162 | loss: 1.135039, accuracy: 56.48%\n",
            "step:  163 | loss: 1.132074, accuracy: 56.56%\n",
            "step:  164 | loss: 1.126172, accuracy: 56.80%\n",
            "step:  165 | loss: 1.122613, accuracy: 56.91%\n",
            "step:  166 | loss: 1.121766, accuracy: 56.96%\n",
            "step:  167 | loss: 1.117762, accuracy: 57.16%\n",
            "step:  168 | loss: 1.114705, accuracy: 57.29%\n",
            "step:  169 | loss: 1.111282, accuracy: 57.46%\n",
            "step:  170 | loss: 1.107965, accuracy: 57.59%\n",
            "step:  171 | loss: 1.105391, accuracy: 57.75%\n",
            "step:  172 | loss: 1.102316, accuracy: 57.94%\n",
            "step:  173 | loss: 1.098784, accuracy: 58.09%\n",
            "step:  174 | loss: 1.095353, accuracy: 58.25%\n",
            "step:  175 | loss: 1.090239, accuracy: 58.46%\n",
            "step:  176 | loss: 1.086710, accuracy: 58.61%\n",
            "step:  177 | loss: 1.084393, accuracy: 58.67%\n",
            "step:  178 | loss: 1.079867, accuracy: 58.88%\n",
            "step:  179 | loss: 1.077182, accuracy: 58.99%\n",
            "step:  180 | loss: 1.074109, accuracy: 59.14%\n",
            "step:  181 | loss: 1.071934, accuracy: 59.23%\n",
            "step:  182 | loss: 1.067930, accuracy: 59.37%\n",
            "step:  183 | loss: 1.066186, accuracy: 59.45%\n",
            "step:  184 | loss: 1.062535, accuracy: 59.59%\n",
            "step:  185 | loss: 1.059679, accuracy: 59.70%\n",
            "step:  186 | loss: 1.056309, accuracy: 59.84%\n",
            "step:  187 | loss: 1.054733, accuracy: 59.89%\n",
            "step:  188 | loss: 1.051208, accuracy: 60.05%\n",
            "step:  189 | loss: 1.048416, accuracy: 60.19%\n",
            "step:  190 | loss: 1.046625, accuracy: 60.32%\n",
            "step:  191 | loss: 1.045152, accuracy: 60.42%\n",
            "step:  192 | loss: 1.042317, accuracy: 60.47%\n",
            "step:  193 | loss: 1.039632, accuracy: 60.57%\n",
            "step:  194 | loss: 1.037630, accuracy: 60.72%\n",
            "step:  195 | loss: 1.034276, accuracy: 60.87%\n",
            "step:  196 | loss: 1.032060, accuracy: 60.94%\n",
            "step:  197 | loss: 1.029013, accuracy: 61.07%\n",
            "step:  198 | loss: 1.027095, accuracy: 61.16%\n",
            "step:  199 | loss: 1.023255, accuracy: 61.33%\n",
            "step:  200 | loss: 1.021471, accuracy: 61.40%\n",
            "step:  201 | loss: 1.018272, accuracy: 61.49%\n",
            "step:  202 | loss: 1.017307, accuracy: 61.53%\n",
            "step:  203 | loss: 1.014653, accuracy: 61.67%\n",
            "step:  204 | loss: 1.010931, accuracy: 61.81%\n",
            "step:  205 | loss: 1.008508, accuracy: 61.93%\n",
            "step:  206 | loss: 1.005800, accuracy: 62.01%\n",
            "step:  207 | loss: 1.003685, accuracy: 62.10%\n",
            "step:  208 | loss: 1.000212, accuracy: 62.28%\n",
            "step:  209 | loss: 0.998164, accuracy: 62.37%\n",
            "step:  210 | loss: 0.995865, accuracy: 62.45%\n",
            "step:  211 | loss: 0.993522, accuracy: 62.56%\n",
            "step:  212 | loss: 0.990693, accuracy: 62.67%\n",
            "step:  213 | loss: 0.988366, accuracy: 62.79%\n",
            "step:  214 | loss: 0.986121, accuracy: 62.85%\n",
            "step:  215 | loss: 0.983000, accuracy: 63.00%\n",
            "step:  216 | loss: 0.979867, accuracy: 63.10%\n",
            "step:  217 | loss: 0.977397, accuracy: 63.23%\n",
            "step:  218 | loss: 0.976638, accuracy: 63.26%\n",
            "step:  219 | loss: 0.973319, accuracy: 63.40%\n",
            "step:  220 | loss: 0.973180, accuracy: 63.43%\n",
            "step:  221 | loss: 0.970667, accuracy: 63.51%\n",
            "step:  222 | loss: 0.967328, accuracy: 63.63%\n",
            "step:  223 | loss: 0.964636, accuracy: 63.74%\n",
            "step:  224 | loss: 0.960834, accuracy: 63.91%\n",
            "step:  225 | loss: 0.957939, accuracy: 63.98%\n",
            "step:  226 | loss: 0.956536, accuracy: 64.05%\n",
            "step:  227 | loss: 0.953983, accuracy: 64.12%\n",
            "step:  228 | loss: 0.950787, accuracy: 64.23%\n",
            "step:  229 | loss: 0.948640, accuracy: 64.30%\n",
            "step:  230 | loss: 0.947214, accuracy: 64.39%\n",
            "step:  231 | loss: 0.944883, accuracy: 64.48%\n",
            "step:  232 | loss: 0.941972, accuracy: 64.59%\n",
            "step:  233 | loss: 0.939239, accuracy: 64.70%\n",
            "step:  234 | loss: 0.936975, accuracy: 64.79%\n",
            "step:  235 | loss: 0.934696, accuracy: 64.85%\n",
            "step:  236 | loss: 0.932964, accuracy: 64.92%\n",
            "step:  237 | loss: 0.931498, accuracy: 65.00%\n",
            "step:  238 | loss: 0.930153, accuracy: 65.08%\n",
            "step:  239 | loss: 0.929563, accuracy: 65.15%\n",
            "step:  240 | loss: 0.927780, accuracy: 65.23%\n",
            "step:  241 | loss: 0.925905, accuracy: 65.31%\n",
            "step:  242 | loss: 0.923864, accuracy: 65.37%\n",
            "step:  243 | loss: 0.921522, accuracy: 65.47%\n",
            "step:  244 | loss: 0.920490, accuracy: 65.51%\n",
            "step:  245 | loss: 0.918663, accuracy: 65.61%\n",
            "step:  246 | loss: 0.916760, accuracy: 65.69%\n",
            "step:  247 | loss: 0.915504, accuracy: 65.73%\n",
            "step:  248 | loss: 0.913783, accuracy: 65.77%\n",
            "step:  249 | loss: 0.912530, accuracy: 65.80%\n",
            "step:  250 | loss: 0.910464, accuracy: 65.88%\n",
            "step:  251 | loss: 0.908272, accuracy: 65.98%\n",
            "step:  252 | loss: 0.906043, accuracy: 66.07%\n",
            "step:  253 | loss: 0.904738, accuracy: 66.15%\n",
            "step:  254 | loss: 0.902025, accuracy: 66.24%\n",
            "step:  255 | loss: 0.899890, accuracy: 66.33%\n",
            "step:  256 | loss: 0.897467, accuracy: 66.43%\n",
            "step:  257 | loss: 0.894290, accuracy: 66.56%\n",
            "step:  258 | loss: 0.892371, accuracy: 66.61%\n",
            "step:  259 | loss: 0.890840, accuracy: 66.66%\n",
            "step:  260 | loss: 0.888590, accuracy: 66.75%\n",
            "step:  261 | loss: 0.886781, accuracy: 66.84%\n",
            "step:  262 | loss: 0.884593, accuracy: 66.93%\n",
            "step:  263 | loss: 0.883798, accuracy: 66.96%\n",
            "step:  264 | loss: 0.881812, accuracy: 67.05%\n",
            "step:  265 | loss: 0.879462, accuracy: 67.13%\n",
            "step:  266 | loss: 0.877447, accuracy: 67.20%\n",
            "step:  267 | loss: 0.876422, accuracy: 67.25%\n",
            "step:  268 | loss: 0.876048, accuracy: 67.28%\n",
            "step:  269 | loss: 0.875055, accuracy: 67.30%\n",
            "step:  270 | loss: 0.873077, accuracy: 67.37%\n",
            "step:  271 | loss: 0.871976, accuracy: 67.42%\n",
            "step:  272 | loss: 0.870433, accuracy: 67.46%\n",
            "step:  273 | loss: 0.867761, accuracy: 67.56%\n",
            "step:  274 | loss: 0.866786, accuracy: 67.61%\n",
            "step:  275 | loss: 0.865222, accuracy: 67.65%\n",
            "step:  276 | loss: 0.863461, accuracy: 67.70%\n",
            "step:  277 | loss: 0.863497, accuracy: 67.67%\n",
            "step:  278 | loss: 0.861313, accuracy: 67.77%\n",
            "step:  279 | loss: 0.861127, accuracy: 67.74%\n",
            "step:  280 | loss: 0.860131, accuracy: 67.75%\n",
            "step:  281 | loss: 0.859547, accuracy: 67.78%\n",
            "step:  282 | loss: 0.858220, accuracy: 67.82%\n",
            "step:  283 | loss: 0.856984, accuracy: 67.86%\n",
            "step:  284 | loss: 0.855093, accuracy: 67.92%\n",
            "step:  285 | loss: 0.854359, accuracy: 67.95%\n",
            "step:  286 | loss: 0.852557, accuracy: 68.04%\n",
            "step:  287 | loss: 0.850637, accuracy: 68.14%\n",
            "step:  288 | loss: 0.848651, accuracy: 68.23%\n",
            "step:  289 | loss: 0.847384, accuracy: 68.29%\n",
            "step:  290 | loss: 0.845751, accuracy: 68.34%\n",
            "step:  291 | loss: 0.846104, accuracy: 68.33%\n",
            "step:  292 | loss: 0.845941, accuracy: 68.39%\n",
            "step:  293 | loss: 0.843526, accuracy: 68.48%\n",
            "step:  294 | loss: 0.841377, accuracy: 68.57%\n",
            "step:  295 | loss: 0.840914, accuracy: 68.63%\n",
            "step:  296 | loss: 0.840300, accuracy: 68.67%\n",
            "step:  297 | loss: 0.837985, accuracy: 68.75%\n",
            "step:  298 | loss: 0.836960, accuracy: 68.79%\n",
            "step:  299 | loss: 0.835672, accuracy: 68.85%\n",
            "step:  300 | loss: 0.834069, accuracy: 68.90%\n",
            "step:  301 | loss: 0.832992, accuracy: 68.97%\n",
            "step:  302 | loss: 0.831204, accuracy: 69.06%\n",
            "step:  303 | loss: 0.829271, accuracy: 69.14%\n",
            "step:  304 | loss: 0.827564, accuracy: 69.21%\n",
            "step:  305 | loss: 0.827279, accuracy: 69.18%\n",
            "step:  306 | loss: 0.825874, accuracy: 69.25%\n",
            "step:  307 | loss: 0.824858, accuracy: 69.30%\n",
            "step:  308 | loss: 0.822914, accuracy: 69.38%\n",
            "step:  309 | loss: 0.821537, accuracy: 69.42%\n",
            "step:  310 | loss: 0.819853, accuracy: 69.48%\n",
            "step:  311 | loss: 0.818428, accuracy: 69.55%\n",
            "step:  312 | loss: 0.816143, accuracy: 69.65%\n",
            "step:  313 | loss: 0.814468, accuracy: 69.73%\n",
            "step:  314 | loss: 0.813427, accuracy: 69.76%\n",
            "step:  315 | loss: 0.812045, accuracy: 69.81%\n",
            "step:  316 | loss: 0.810736, accuracy: 69.86%\n",
            "step:  317 | loss: 0.808775, accuracy: 69.94%\n",
            "step:  318 | loss: 0.808283, accuracy: 69.95%\n",
            "step:  319 | loss: 0.808042, accuracy: 70.02%\n",
            "step:  320 | loss: 0.806344, accuracy: 70.06%\n",
            "step:  321 | loss: 0.805751, accuracy: 70.06%\n",
            "step:  322 | loss: 0.803886, accuracy: 70.14%\n",
            "step:  323 | loss: 0.802932, accuracy: 70.14%\n",
            "step:  324 | loss: 0.802333, accuracy: 70.17%\n",
            "step:  325 | loss: 0.801538, accuracy: 70.18%\n",
            "step:  326 | loss: 0.799700, accuracy: 70.26%\n",
            "step:  327 | loss: 0.798374, accuracy: 70.32%\n",
            "step:  328 | loss: 0.797013, accuracy: 70.35%\n",
            "step:  329 | loss: 0.795747, accuracy: 70.40%\n",
            "step:  330 | loss: 0.794344, accuracy: 70.45%\n",
            "step:  331 | loss: 0.792451, accuracy: 70.53%\n",
            "step:  332 | loss: 0.791428, accuracy: 70.56%\n",
            "step:  333 | loss: 0.790557, accuracy: 70.57%\n",
            "step:  334 | loss: 0.789011, accuracy: 70.63%\n",
            "step:  335 | loss: 0.788342, accuracy: 70.67%\n",
            "step:  336 | loss: 0.787117, accuracy: 70.71%\n",
            "step:  337 | loss: 0.785713, accuracy: 70.79%\n",
            "step:  338 | loss: 0.784050, accuracy: 70.86%\n",
            "step:  339 | loss: 0.782734, accuracy: 70.90%\n",
            "step:  340 | loss: 0.781142, accuracy: 70.97%\n",
            "step:  341 | loss: 0.779064, accuracy: 71.06%\n",
            "step:  342 | loss: 0.778358, accuracy: 71.08%\n",
            "step:  343 | loss: 0.778110, accuracy: 71.08%\n",
            "step:  344 | loss: 0.776783, accuracy: 71.13%\n",
            "step:  345 | loss: 0.775073, accuracy: 71.20%\n",
            "step:  346 | loss: 0.773281, accuracy: 71.27%\n",
            "step:  347 | loss: 0.771807, accuracy: 71.34%\n",
            "step:  348 | loss: 0.771523, accuracy: 71.38%\n",
            "step:  349 | loss: 0.769718, accuracy: 71.46%\n",
            "step:  350 | loss: 0.768480, accuracy: 71.51%\n",
            "step:  351 | loss: 0.767130, accuracy: 71.57%\n",
            "step:  352 | loss: 0.766209, accuracy: 71.61%\n",
            "step:  353 | loss: 0.765513, accuracy: 71.61%\n",
            "step:  354 | loss: 0.764433, accuracy: 71.67%\n",
            "step:  355 | loss: 0.763234, accuracy: 71.73%\n",
            "step:  356 | loss: 0.761736, accuracy: 71.80%\n",
            "step:  357 | loss: 0.760924, accuracy: 71.83%\n",
            "step:  358 | loss: 0.759422, accuracy: 71.89%\n",
            "step:  359 | loss: 0.758745, accuracy: 71.89%\n",
            "step:  360 | loss: 0.757328, accuracy: 71.93%\n",
            "step:  361 | loss: 0.757346, accuracy: 71.97%\n",
            "step:  362 | loss: 0.756794, accuracy: 71.98%\n",
            "step:  363 | loss: 0.755534, accuracy: 72.01%\n",
            "step:  364 | loss: 0.754780, accuracy: 72.03%\n",
            "step:  365 | loss: 0.753818, accuracy: 72.07%\n",
            "step:  366 | loss: 0.752487, accuracy: 72.13%\n",
            "step:  367 | loss: 0.751898, accuracy: 72.17%\n",
            "step:  368 | loss: 0.751331, accuracy: 72.19%\n",
            "step:  369 | loss: 0.750265, accuracy: 72.22%\n",
            "step:  370 | loss: 0.748503, accuracy: 72.28%\n",
            "step:  371 | loss: 0.746861, accuracy: 72.36%\n",
            "step:  372 | loss: 0.745478, accuracy: 72.42%\n",
            "step:  373 | loss: 0.744714, accuracy: 72.43%\n",
            "step:  374 | loss: 0.743813, accuracy: 72.45%\n",
            "step:  375 | loss: 0.742940, accuracy: 72.47%\n",
            "step:  376 | loss: 0.741942, accuracy: 72.51%\n",
            "step:  377 | loss: 0.740936, accuracy: 72.56%\n",
            "step:  378 | loss: 0.739223, accuracy: 72.63%\n",
            "step:  379 | loss: 0.738164, accuracy: 72.68%\n",
            "step:  380 | loss: 0.737057, accuracy: 72.72%\n",
            "step:  381 | loss: 0.735733, accuracy: 72.77%\n",
            "step:  382 | loss: 0.734154, accuracy: 72.84%\n",
            "step:  383 | loss: 0.732992, accuracy: 72.90%\n",
            "step:  384 | loss: 0.731279, accuracy: 72.97%\n",
            "step:  385 | loss: 0.730020, accuracy: 73.03%\n",
            "step:  386 | loss: 0.730111, accuracy: 73.04%\n",
            "step:  387 | loss: 0.729678, accuracy: 73.06%\n",
            "step:  388 | loss: 0.728938, accuracy: 73.09%\n",
            "step:  389 | loss: 0.728671, accuracy: 73.08%\n",
            "step:  390 | loss: 0.727477, accuracy: 73.14%\n",
            "step:  391 | loss: 0.726254, accuracy: 73.18%\n",
            "step:  392 | loss: 0.725279, accuracy: 73.21%\n",
            "step:  393 | loss: 0.724299, accuracy: 73.23%\n",
            "step:  394 | loss: 0.723579, accuracy: 73.27%\n",
            "step:  395 | loss: 0.722793, accuracy: 73.32%\n",
            "step:  396 | loss: 0.721914, accuracy: 73.35%\n",
            "step:  397 | loss: 0.720594, accuracy: 73.39%\n",
            "step:  398 | loss: 0.719457, accuracy: 73.42%\n",
            "step:  399 | loss: 0.718684, accuracy: 73.43%\n",
            "step:  400 | loss: 0.717391, accuracy: 73.49%\n",
            "step:  401 | loss: 0.715863, accuracy: 73.55%\n",
            "step:  402 | loss: 0.714505, accuracy: 73.61%\n",
            "step:  403 | loss: 0.713031, accuracy: 73.66%\n",
            "step:  404 | loss: 0.711773, accuracy: 73.73%\n",
            "step:  405 | loss: 0.711026, accuracy: 73.72%\n",
            "step:  406 | loss: 0.710398, accuracy: 73.73%\n",
            "step:  407 | loss: 0.709435, accuracy: 73.77%\n",
            "step:  408 | loss: 0.708371, accuracy: 73.82%\n",
            "step:  409 | loss: 0.707201, accuracy: 73.88%\n",
            "step:  410 | loss: 0.705863, accuracy: 73.93%\n",
            "step:  411 | loss: 0.704808, accuracy: 73.95%\n",
            "step:  412 | loss: 0.704053, accuracy: 73.99%\n",
            "step:  413 | loss: 0.704242, accuracy: 74.01%\n",
            "step:  414 | loss: 0.703081, accuracy: 74.03%\n",
            "step:  415 | loss: 0.701599, accuracy: 74.10%\n",
            "step:  416 | loss: 0.700537, accuracy: 74.15%\n",
            "step:  417 | loss: 0.699962, accuracy: 74.18%\n",
            "step:  418 | loss: 0.699176, accuracy: 74.22%\n",
            "step:  419 | loss: 0.697915, accuracy: 74.27%\n",
            "step:  420 | loss: 0.696540, accuracy: 74.33%\n",
            "step:  421 | loss: 0.695392, accuracy: 74.38%\n",
            "step:  422 | loss: 0.694749, accuracy: 74.38%\n",
            "step:  423 | loss: 0.693662, accuracy: 74.42%\n",
            "step:  424 | loss: 0.692722, accuracy: 74.46%\n",
            "step:  425 | loss: 0.691612, accuracy: 74.51%\n",
            "step:  426 | loss: 0.690437, accuracy: 74.55%\n",
            "step:  427 | loss: 0.688991, accuracy: 74.61%\n",
            "step:  428 | loss: 0.687847, accuracy: 74.65%\n",
            "step:  429 | loss: 0.687024, accuracy: 74.66%\n",
            "step:  430 | loss: 0.685739, accuracy: 74.72%\n",
            "step:  431 | loss: 0.685265, accuracy: 74.74%\n",
            "step:  432 | loss: 0.684341, accuracy: 74.78%\n",
            "step:  433 | loss: 0.683531, accuracy: 74.83%\n",
            "step:  434 | loss: 0.683008, accuracy: 74.84%\n",
            "step:  435 | loss: 0.681655, accuracy: 74.89%\n",
            "step:  436 | loss: 0.680495, accuracy: 74.92%\n",
            "step:  437 | loss: 0.679565, accuracy: 74.95%\n",
            "step:  438 | loss: 0.678548, accuracy: 75.00%\n",
            "step:  439 | loss: 0.677637, accuracy: 75.03%\n",
            "step:  440 | loss: 0.676544, accuracy: 75.08%\n",
            "step:  441 | loss: 0.675645, accuracy: 75.11%\n",
            "step:  442 | loss: 0.675404, accuracy: 75.12%\n",
            "step:  443 | loss: 0.674245, accuracy: 75.17%\n",
            "step:  444 | loss: 0.672788, accuracy: 75.23%\n",
            "step:  445 | loss: 0.671440, accuracy: 75.28%\n",
            "step:  446 | loss: 0.670434, accuracy: 75.31%\n",
            "step:  447 | loss: 0.670419, accuracy: 75.31%\n",
            "step:  448 | loss: 0.670467, accuracy: 75.33%\n",
            "step:  449 | loss: 0.669333, accuracy: 75.38%\n",
            "step:  450 | loss: 0.668834, accuracy: 75.41%\n",
            "step:  451 | loss: 0.668125, accuracy: 75.42%\n",
            "step:  452 | loss: 0.667335, accuracy: 75.44%\n",
            "step:  453 | loss: 0.666408, accuracy: 75.47%\n",
            "step:  454 | loss: 0.665652, accuracy: 75.51%\n",
            "step:  455 | loss: 0.665216, accuracy: 75.51%\n",
            "step:  456 | loss: 0.663885, accuracy: 75.56%\n",
            "step:  457 | loss: 0.663654, accuracy: 75.56%\n",
            "step:  458 | loss: 0.662802, accuracy: 75.60%\n",
            "step:  459 | loss: 0.661687, accuracy: 75.65%\n",
            "step:  460 | loss: 0.661218, accuracy: 75.67%\n",
            "step:  461 | loss: 0.660247, accuracy: 75.70%\n",
            "step:  462 | loss: 0.659819, accuracy: 75.73%\n",
            "step:  463 | loss: 0.658931, accuracy: 75.77%\n",
            "step:  464 | loss: 0.658777, accuracy: 75.78%\n",
            "step:  465 | loss: 0.657614, accuracy: 75.82%\n",
            "step:  466 | loss: 0.656435, accuracy: 75.86%\n",
            "step:  467 | loss: 0.655797, accuracy: 75.90%\n",
            "step:  468 | loss: 0.655183, accuracy: 75.94%\n",
            "step:  469 | loss: 0.655266, accuracy: 75.95%\n",
            "step:  470 | loss: 0.654841, accuracy: 75.97%\n",
            "step:  471 | loss: 0.653860, accuracy: 76.01%\n",
            "step:  472 | loss: 0.652612, accuracy: 76.06%\n",
            "step:  473 | loss: 0.652156, accuracy: 76.08%\n",
            "step:  474 | loss: 0.651233, accuracy: 76.12%\n",
            "step:  475 | loss: 0.650441, accuracy: 76.15%\n",
            "step:  476 | loss: 0.649924, accuracy: 76.17%\n",
            "step:  477 | loss: 0.649030, accuracy: 76.21%\n",
            "step:  478 | loss: 0.648365, accuracy: 76.22%\n",
            "step:  479 | loss: 0.647433, accuracy: 76.25%\n",
            "step:  480 | loss: 0.646955, accuracy: 76.27%\n",
            "step:  481 | loss: 0.645783, accuracy: 76.32%\n",
            "step:  482 | loss: 0.644998, accuracy: 76.36%\n",
            "step:  483 | loss: 0.644308, accuracy: 76.36%\n",
            "step:  484 | loss: 0.643231, accuracy: 76.39%\n",
            "step:  485 | loss: 0.642213, accuracy: 76.43%\n",
            "step:  486 | loss: 0.641374, accuracy: 76.45%\n",
            "step:  487 | loss: 0.640901, accuracy: 76.48%\n",
            "step:  488 | loss: 0.640178, accuracy: 76.51%\n",
            "step:  489 | loss: 0.639090, accuracy: 76.55%\n",
            "step:  490 | loss: 0.638887, accuracy: 76.58%\n",
            "step:  491 | loss: 0.638521, accuracy: 76.59%\n",
            "step:  492 | loss: 0.637718, accuracy: 76.62%\n",
            "step:  493 | loss: 0.637162, accuracy: 76.64%\n",
            "step:  494 | loss: 0.636115, accuracy: 76.69%\n",
            "step:  495 | loss: 0.636610, accuracy: 76.69%\n",
            "step:  496 | loss: 0.635985, accuracy: 76.70%\n",
            "step:  497 | loss: 0.635162, accuracy: 76.74%\n",
            "step:  498 | loss: 0.634574, accuracy: 76.78%\n",
            "step:  499 | loss: 0.633610, accuracy: 76.81%\n",
            "step:  500 | loss: 0.633268, accuracy: 76.83%\n",
            "step:  501 | loss: 0.632639, accuracy: 76.85%\n",
            "step:  502 | loss: 0.631984, accuracy: 76.86%\n",
            "step:  503 | loss: 0.630957, accuracy: 76.91%\n",
            "step:  504 | loss: 0.630211, accuracy: 76.93%\n",
            "step:  505 | loss: 0.629741, accuracy: 76.96%\n",
            "step:  506 | loss: 0.629087, accuracy: 76.97%\n",
            "step:  507 | loss: 0.628224, accuracy: 76.99%\n",
            "step:  508 | loss: 0.627477, accuracy: 77.03%\n",
            "step:  509 | loss: 0.626920, accuracy: 77.05%\n",
            "step:  510 | loss: 0.626554, accuracy: 77.06%\n",
            "step:  511 | loss: 0.625592, accuracy: 77.09%\n",
            "step:  512 | loss: 0.624634, accuracy: 77.13%\n",
            "step:  513 | loss: 0.625010, accuracy: 77.13%\n",
            "step:  514 | loss: 0.624541, accuracy: 77.15%\n",
            "step:  515 | loss: 0.623641, accuracy: 77.18%\n",
            "step:  516 | loss: 0.623369, accuracy: 77.20%\n",
            "step:  517 | loss: 0.622257, accuracy: 77.24%\n",
            "step:  518 | loss: 0.621672, accuracy: 77.27%\n",
            "step:  519 | loss: 0.621158, accuracy: 77.28%\n",
            "step:  520 | loss: 0.620622, accuracy: 77.29%\n",
            "step:  521 | loss: 0.620504, accuracy: 77.28%\n",
            "step:  522 | loss: 0.619607, accuracy: 77.31%\n",
            "step:  523 | loss: 0.618688, accuracy: 77.34%\n",
            "step:  524 | loss: 0.617724, accuracy: 77.39%\n",
            "step:  525 | loss: 0.616909, accuracy: 77.41%\n",
            "step:  526 | loss: 0.616030, accuracy: 77.44%\n",
            "step:  527 | loss: 0.616211, accuracy: 77.44%\n",
            "step:  528 | loss: 0.615072, accuracy: 77.48%\n",
            "step:  529 | loss: 0.614005, accuracy: 77.52%\n",
            "step:  530 | loss: 0.613556, accuracy: 77.55%\n",
            "step:  531 | loss: 0.612824, accuracy: 77.57%\n",
            "step:  532 | loss: 0.611956, accuracy: 77.60%\n",
            "step:  533 | loss: 0.611182, accuracy: 77.64%\n",
            "step:  534 | loss: 0.611306, accuracy: 77.64%\n",
            "step:  535 | loss: 0.610761, accuracy: 77.66%\n",
            "step:  536 | loss: 0.610198, accuracy: 77.70%\n",
            "step:  537 | loss: 0.609691, accuracy: 77.72%\n",
            "step:  538 | loss: 0.608988, accuracy: 77.75%\n",
            "step:  539 | loss: 0.608751, accuracy: 77.76%\n",
            "step:  540 | loss: 0.608386, accuracy: 77.78%\n",
            "step:  541 | loss: 0.608310, accuracy: 77.77%\n",
            "step:  542 | loss: 0.607358, accuracy: 77.80%\n",
            "step:  543 | loss: 0.606465, accuracy: 77.84%\n",
            "step:  544 | loss: 0.606301, accuracy: 77.84%\n",
            "step:  545 | loss: 0.605673, accuracy: 77.86%\n",
            "step:  546 | loss: 0.604785, accuracy: 77.89%\n",
            "step:  547 | loss: 0.604225, accuracy: 77.92%\n",
            "step:  548 | loss: 0.603563, accuracy: 77.95%\n",
            "step:  549 | loss: 0.603022, accuracy: 77.97%\n",
            "step:  550 | loss: 0.602429, accuracy: 78.00%\n",
            "step:  551 | loss: 0.601522, accuracy: 78.03%\n",
            "step:  552 | loss: 0.600713, accuracy: 78.06%\n",
            "step:  553 | loss: 0.599966, accuracy: 78.08%\n",
            "step:  554 | loss: 0.598982, accuracy: 78.12%\n",
            "step:  555 | loss: 0.598733, accuracy: 78.14%\n",
            "step:  556 | loss: 0.597813, accuracy: 78.17%\n",
            "step:  557 | loss: 0.596969, accuracy: 78.20%\n",
            "step:  558 | loss: 0.597142, accuracy: 78.21%\n",
            "step:  559 | loss: 0.596541, accuracy: 78.23%\n",
            "step:  560 | loss: 0.595765, accuracy: 78.26%\n",
            "step:  561 | loss: 0.595000, accuracy: 78.29%\n",
            "step:  562 | loss: 0.594252, accuracy: 78.32%\n",
            "step:  563 | loss: 0.593451, accuracy: 78.35%\n",
            "step:  564 | loss: 0.592648, accuracy: 78.38%\n",
            "step:  565 | loss: 0.592057, accuracy: 78.40%\n",
            "step:  566 | loss: 0.591184, accuracy: 78.44%\n",
            "step:  567 | loss: 0.590316, accuracy: 78.47%\n",
            "step:  568 | loss: 0.590387, accuracy: 78.45%\n",
            "step:  569 | loss: 0.589804, accuracy: 78.46%\n",
            "step:  570 | loss: 0.589276, accuracy: 78.48%\n",
            "step:  571 | loss: 0.588343, accuracy: 78.52%\n",
            "step:  572 | loss: 0.588584, accuracy: 78.51%\n",
            "step:  573 | loss: 0.588024, accuracy: 78.53%\n",
            "step:  574 | loss: 0.587151, accuracy: 78.56%\n",
            "step:  575 | loss: 0.586357, accuracy: 78.59%\n",
            "step:  576 | loss: 0.585846, accuracy: 78.60%\n",
            "step:  577 | loss: 0.585928, accuracy: 78.61%\n",
            "step:  578 | loss: 0.584998, accuracy: 78.65%\n",
            "step:  579 | loss: 0.585003, accuracy: 78.66%\n",
            "step:  580 | loss: 0.584288, accuracy: 78.68%\n",
            "step:  581 | loss: 0.583945, accuracy: 78.68%\n",
            "step:  582 | loss: 0.583872, accuracy: 78.68%\n",
            "step:  583 | loss: 0.583685, accuracy: 78.67%\n",
            "step:  584 | loss: 0.583251, accuracy: 78.68%\n",
            "step:  585 | loss: 0.582422, accuracy: 78.71%\n",
            "step:  586 | loss: 0.581847, accuracy: 78.72%\n",
            "step:  587 | loss: 0.581534, accuracy: 78.74%\n",
            "step:  588 | loss: 0.581093, accuracy: 78.76%\n",
            "step:  589 | loss: 0.580170, accuracy: 78.79%\n",
            "step:  590 | loss: 0.579390, accuracy: 78.82%\n",
            "step:  591 | loss: 0.578711, accuracy: 78.85%\n",
            "step:  592 | loss: 0.578226, accuracy: 78.86%\n",
            "step:  593 | loss: 0.577569, accuracy: 78.88%\n",
            "step:  594 | loss: 0.577216, accuracy: 78.90%\n",
            "step:  595 | loss: 0.576584, accuracy: 78.92%\n",
            "step:  596 | loss: 0.575989, accuracy: 78.93%\n",
            "step:  597 | loss: 0.575845, accuracy: 78.94%\n",
            "step:  598 | loss: 0.575148, accuracy: 78.96%\n",
            "step:  599 | loss: 0.575053, accuracy: 78.96%\n",
            "step:  600 | loss: 0.575217, accuracy: 78.94%\n",
            "step:  601 | loss: 0.574818, accuracy: 78.94%\n",
            "step:  602 | loss: 0.573974, accuracy: 78.98%\n",
            "step:  603 | loss: 0.573817, accuracy: 78.98%\n",
            "step:  604 | loss: 0.573193, accuracy: 79.01%\n",
            "step:  605 | loss: 0.572915, accuracy: 79.01%\n",
            "step:  606 | loss: 0.572303, accuracy: 79.03%\n",
            "step:  607 | loss: 0.571826, accuracy: 79.04%\n",
            "step:  608 | loss: 0.571973, accuracy: 79.05%\n",
            "step:  609 | loss: 0.571550, accuracy: 79.06%\n",
            "step:  610 | loss: 0.571162, accuracy: 79.07%\n",
            "step:  611 | loss: 0.570685, accuracy: 79.08%\n",
            "step:  612 | loss: 0.570386, accuracy: 79.09%\n",
            "step:  613 | loss: 0.570132, accuracy: 79.10%\n",
            "step:  614 | loss: 0.569807, accuracy: 79.12%\n",
            "step:  615 | loss: 0.569794, accuracy: 79.12%\n",
            "step:  616 | loss: 0.569206, accuracy: 79.14%\n",
            "step:  617 | loss: 0.568738, accuracy: 79.17%\n",
            "step:  618 | loss: 0.568453, accuracy: 79.18%\n",
            "step:  619 | loss: 0.567634, accuracy: 79.22%\n",
            "step:  620 | loss: 0.567691, accuracy: 79.21%\n",
            "step:  621 | loss: 0.566888, accuracy: 79.24%\n",
            "step:  622 | loss: 0.566057, accuracy: 79.28%\n",
            "step:  623 | loss: 0.565356, accuracy: 79.30%\n",
            "step:  624 | loss: 0.564712, accuracy: 79.33%\n",
            "step:  625 | loss: 0.564423, accuracy: 79.33%\n",
            "step:  626 | loss: 0.563940, accuracy: 79.35%\n",
            "step:  627 | loss: 0.563478, accuracy: 79.37%\n",
            "step:  628 | loss: 0.562952, accuracy: 79.39%\n",
            "step:  629 | loss: 0.562673, accuracy: 79.40%\n",
            "step:  630 | loss: 0.562226, accuracy: 79.41%\n",
            "step:  631 | loss: 0.561892, accuracy: 79.41%\n",
            "step:  632 | loss: 0.561137, accuracy: 79.43%\n",
            "step:  633 | loss: 0.560763, accuracy: 79.45%\n",
            "step:  634 | loss: 0.560205, accuracy: 79.47%\n",
            "step:  635 | loss: 0.559960, accuracy: 79.50%\n",
            "step:  636 | loss: 0.559177, accuracy: 79.53%\n",
            "step:  637 | loss: 0.558711, accuracy: 79.55%\n",
            "step:  638 | loss: 0.558101, accuracy: 79.58%\n",
            "step:  639 | loss: 0.557499, accuracy: 79.60%\n",
            "step:  640 | loss: 0.557187, accuracy: 79.60%\n",
            "step:  641 | loss: 0.556607, accuracy: 79.62%\n",
            "step:  642 | loss: 0.556310, accuracy: 79.62%\n",
            "step:  643 | loss: 0.556596, accuracy: 79.62%\n",
            "step:  644 | loss: 0.556195, accuracy: 79.64%\n",
            "step:  645 | loss: 0.555739, accuracy: 79.65%\n",
            "step:  646 | loss: 0.554960, accuracy: 79.68%\n",
            "step:  647 | loss: 0.554422, accuracy: 79.70%\n",
            "step:  648 | loss: 0.554056, accuracy: 79.72%\n",
            "step:  649 | loss: 0.553327, accuracy: 79.75%\n",
            "step:  650 | loss: 0.552526, accuracy: 79.78%\n",
            "step:  651 | loss: 0.552300, accuracy: 79.78%\n",
            "step:  652 | loss: 0.552086, accuracy: 79.79%\n",
            "step:  653 | loss: 0.551673, accuracy: 79.82%\n",
            "step:  654 | loss: 0.551160, accuracy: 79.83%\n",
            "step:  655 | loss: 0.550477, accuracy: 79.85%\n",
            "step:  656 | loss: 0.549865, accuracy: 79.87%\n",
            "step:  657 | loss: 0.549557, accuracy: 79.89%\n",
            "step:  658 | loss: 0.549290, accuracy: 79.89%\n",
            "step:  659 | loss: 0.549077, accuracy: 79.89%\n",
            "step:  660 | loss: 0.548515, accuracy: 79.92%\n",
            "step:  661 | loss: 0.547964, accuracy: 79.94%\n",
            "step:  662 | loss: 0.548017, accuracy: 79.94%\n",
            "step:  663 | loss: 0.547972, accuracy: 79.95%\n",
            "step:  664 | loss: 0.547411, accuracy: 79.97%\n",
            "step:  665 | loss: 0.546600, accuracy: 80.00%\n",
            "step:  666 | loss: 0.546104, accuracy: 80.02%\n",
            "step:  667 | loss: 0.545500, accuracy: 80.04%\n",
            "step:  668 | loss: 0.545014, accuracy: 80.07%\n",
            "step:  669 | loss: 0.544381, accuracy: 80.09%\n",
            "step:  670 | loss: 0.543868, accuracy: 80.11%\n",
            "step:  671 | loss: 0.543360, accuracy: 80.13%\n",
            "step:  672 | loss: 0.542689, accuracy: 80.15%\n",
            "step:  673 | loss: 0.542469, accuracy: 80.16%\n",
            "step:  674 | loss: 0.541948, accuracy: 80.18%\n",
            "step:  675 | loss: 0.541923, accuracy: 80.16%\n",
            "step:  676 | loss: 0.541266, accuracy: 80.18%\n",
            "step:  677 | loss: 0.540611, accuracy: 80.21%\n",
            "step:  678 | loss: 0.540207, accuracy: 80.23%\n",
            "step:  679 | loss: 0.539578, accuracy: 80.25%\n",
            "step:  680 | loss: 0.539320, accuracy: 80.27%\n",
            "step:  681 | loss: 0.538637, accuracy: 80.30%\n",
            "step:  682 | loss: 0.538138, accuracy: 80.32%\n",
            "step:  683 | loss: 0.537600, accuracy: 80.33%\n",
            "step:  684 | loss: 0.536956, accuracy: 80.35%\n",
            "step:  685 | loss: 0.536403, accuracy: 80.37%\n",
            "step:  686 | loss: 0.535831, accuracy: 80.39%\n",
            "step:  687 | loss: 0.535492, accuracy: 80.40%\n",
            "step:  688 | loss: 0.534975, accuracy: 80.42%\n",
            "step:  689 | loss: 0.534319, accuracy: 80.45%\n",
            "step:  690 | loss: 0.533588, accuracy: 80.48%\n",
            "step:  691 | loss: 0.533236, accuracy: 80.48%\n",
            "step:  692 | loss: 0.532782, accuracy: 80.51%\n",
            "step:  693 | loss: 0.532417, accuracy: 80.52%\n",
            "step:  694 | loss: 0.531860, accuracy: 80.54%\n",
            "step:  695 | loss: 0.531687, accuracy: 80.54%\n",
            "step:  696 | loss: 0.531189, accuracy: 80.55%\n",
            "step:  697 | loss: 0.530631, accuracy: 80.57%\n",
            "step:  698 | loss: 0.530835, accuracy: 80.57%\n",
            "step:  699 | loss: 0.530513, accuracy: 80.58%\n",
            "step:  700 | loss: 0.529876, accuracy: 80.61%\n",
            "step:  701 | loss: 0.529282, accuracy: 80.63%\n",
            "step:  702 | loss: 0.528709, accuracy: 80.66%\n",
            "step:  703 | loss: 0.528081, accuracy: 80.68%\n",
            "step:  704 | loss: 0.527721, accuracy: 80.69%\n",
            "step:  705 | loss: 0.527427, accuracy: 80.70%\n",
            "step:  706 | loss: 0.527488, accuracy: 80.69%\n",
            "step:  707 | loss: 0.527136, accuracy: 80.71%\n",
            "step:  708 | loss: 0.526658, accuracy: 80.72%\n",
            "step:  709 | loss: 0.526035, accuracy: 80.75%\n",
            "step:  710 | loss: 0.525924, accuracy: 80.76%\n",
            "step:  711 | loss: 0.525644, accuracy: 80.77%\n",
            "step:  712 | loss: 0.525250, accuracy: 80.78%\n",
            "step:  713 | loss: 0.524699, accuracy: 80.81%\n",
            "step:  714 | loss: 0.524415, accuracy: 80.83%\n",
            "step:  715 | loss: 0.524006, accuracy: 80.84%\n",
            "step:  716 | loss: 0.523791, accuracy: 80.84%\n",
            "step:  717 | loss: 0.523278, accuracy: 80.86%\n",
            "step:  718 | loss: 0.522836, accuracy: 80.88%\n",
            "step:  719 | loss: 0.522618, accuracy: 80.88%\n",
            "step:  720 | loss: 0.522298, accuracy: 80.90%\n",
            "step:  721 | loss: 0.522313, accuracy: 80.91%\n",
            "step:  722 | loss: 0.522364, accuracy: 80.91%\n",
            "step:  723 | loss: 0.522239, accuracy: 80.91%\n",
            "step:  724 | loss: 0.521710, accuracy: 80.93%\n",
            "step:  725 | loss: 0.521070, accuracy: 80.96%\n",
            "step:  726 | loss: 0.520958, accuracy: 80.96%\n",
            "step:  727 | loss: 0.520409, accuracy: 80.98%\n",
            "step:  728 | loss: 0.520284, accuracy: 80.98%\n",
            "step:  729 | loss: 0.519685, accuracy: 81.01%\n",
            "step:  730 | loss: 0.519391, accuracy: 81.01%\n",
            "step:  731 | loss: 0.518920, accuracy: 81.03%\n",
            "step:  732 | loss: 0.518448, accuracy: 81.05%\n",
            "step:  733 | loss: 0.518177, accuracy: 81.06%\n",
            "step:  734 | loss: 0.518155, accuracy: 81.06%\n",
            "step:  735 | loss: 0.517844, accuracy: 81.07%\n",
            "step:  736 | loss: 0.517762, accuracy: 81.08%\n",
            "step:  737 | loss: 0.517391, accuracy: 81.10%\n",
            "step:  738 | loss: 0.517017, accuracy: 81.11%\n",
            "step:  739 | loss: 0.516471, accuracy: 81.13%\n",
            "step:  740 | loss: 0.516209, accuracy: 81.14%\n",
            "step:  741 | loss: 0.515686, accuracy: 81.15%\n",
            "step:  742 | loss: 0.515232, accuracy: 81.17%\n",
            "step:  743 | loss: 0.515156, accuracy: 81.18%\n",
            "step:  744 | loss: 0.514754, accuracy: 81.19%\n",
            "step:  745 | loss: 0.514446, accuracy: 81.19%\n",
            "step:  746 | loss: 0.514027, accuracy: 81.21%\n",
            "step:  747 | loss: 0.513482, accuracy: 81.24%\n",
            "step:  748 | loss: 0.513507, accuracy: 81.24%\n",
            "step:  749 | loss: 0.513362, accuracy: 81.24%\n",
            "step:  750 | loss: 0.512896, accuracy: 81.26%\n",
            "step:  751 | loss: 0.512568, accuracy: 81.26%\n",
            "step:  752 | loss: 0.512204, accuracy: 81.28%\n",
            "step:  753 | loss: 0.511875, accuracy: 81.29%\n",
            "step:  754 | loss: 0.511262, accuracy: 81.32%\n",
            "step:  755 | loss: 0.510902, accuracy: 81.33%\n",
            "step:  756 | loss: 0.511054, accuracy: 81.32%\n",
            "step:  757 | loss: 0.510908, accuracy: 81.33%\n",
            "step:  758 | loss: 0.510590, accuracy: 81.35%\n",
            "step:  759 | loss: 0.510089, accuracy: 81.37%\n",
            "step:  760 | loss: 0.510522, accuracy: 81.36%\n",
            "step:  761 | loss: 0.510469, accuracy: 81.37%\n",
            "step:  762 | loss: 0.510084, accuracy: 81.37%\n",
            "step:  763 | loss: 0.509486, accuracy: 81.40%\n",
            "step:  764 | loss: 0.508974, accuracy: 81.42%\n",
            "step:  765 | loss: 0.508840, accuracy: 81.42%\n",
            "step:  766 | loss: 0.508316, accuracy: 81.44%\n",
            "step:  767 | loss: 0.508008, accuracy: 81.45%\n",
            "step:  768 | loss: 0.507432, accuracy: 81.47%\n",
            "step:  769 | loss: 0.506835, accuracy: 81.50%\n",
            "step:  770 | loss: 0.506358, accuracy: 81.51%\n",
            "step:  771 | loss: 0.506398, accuracy: 81.51%\n",
            "step:  772 | loss: 0.506275, accuracy: 81.50%\n",
            "step:  773 | loss: 0.505954, accuracy: 81.51%\n",
            "step:  774 | loss: 0.505740, accuracy: 81.52%\n",
            "step:  775 | loss: 0.505226, accuracy: 81.54%\n",
            "step:  776 | loss: 0.504952, accuracy: 81.54%\n",
            "step:  777 | loss: 0.504708, accuracy: 81.54%\n",
            "step:  778 | loss: 0.504219, accuracy: 81.57%\n",
            "step:  779 | loss: 0.504035, accuracy: 81.58%\n",
            "step:  780 | loss: 0.503622, accuracy: 81.60%\n",
            "step:  781 | loss: 0.503517, accuracy: 81.61%\n",
            "step:  782 | loss: 0.503380, accuracy: 81.62%\n",
            "step:  783 | loss: 0.502838, accuracy: 81.64%\n",
            "step:  784 | loss: 0.502553, accuracy: 81.65%\n",
            "step:  785 | loss: 0.502071, accuracy: 81.66%\n",
            "step:  786 | loss: 0.501627, accuracy: 81.68%\n",
            "step:  787 | loss: 0.501178, accuracy: 81.70%\n",
            "step:  788 | loss: 0.500595, accuracy: 81.72%\n",
            "step:  789 | loss: 0.500625, accuracy: 81.73%\n",
            "step:  790 | loss: 0.500359, accuracy: 81.73%\n",
            "step:  791 | loss: 0.500168, accuracy: 81.74%\n",
            "step:  792 | loss: 0.499748, accuracy: 81.76%\n",
            "step:  793 | loss: 0.499520, accuracy: 81.75%\n",
            "step:  794 | loss: 0.499164, accuracy: 81.77%\n",
            "step:  795 | loss: 0.498655, accuracy: 81.79%\n",
            "step:  796 | loss: 0.498332, accuracy: 81.80%\n",
            "step:  797 | loss: 0.497891, accuracy: 81.82%\n",
            "step:  798 | loss: 0.497657, accuracy: 81.82%\n",
            "step:  799 | loss: 0.497236, accuracy: 81.84%\n",
            "step:  800 | loss: 0.496684, accuracy: 81.86%\n",
            "step:  801 | loss: 0.496383, accuracy: 81.87%\n",
            "step:  802 | loss: 0.496039, accuracy: 81.88%\n",
            "step:  803 | loss: 0.495826, accuracy: 81.89%\n",
            "step:  804 | loss: 0.495666, accuracy: 81.90%\n",
            "step:  805 | loss: 0.495357, accuracy: 81.91%\n",
            "step:  806 | loss: 0.494933, accuracy: 81.92%\n",
            "step:  807 | loss: 0.494436, accuracy: 81.95%\n",
            "step:  808 | loss: 0.493957, accuracy: 81.97%\n",
            "step:  809 | loss: 0.493488, accuracy: 81.99%\n",
            "step:  810 | loss: 0.493132, accuracy: 82.01%\n",
            "step:  811 | loss: 0.492946, accuracy: 82.01%\n",
            "step:  812 | loss: 0.493099, accuracy: 82.01%\n",
            "step:  813 | loss: 0.492662, accuracy: 82.04%\n",
            "step:  814 | loss: 0.492376, accuracy: 82.04%\n",
            "step:  815 | loss: 0.491942, accuracy: 82.06%\n",
            "step:  816 | loss: 0.491602, accuracy: 82.06%\n",
            "step:  817 | loss: 0.491222, accuracy: 82.07%\n",
            "step:  818 | loss: 0.490699, accuracy: 82.10%\n",
            "step:  819 | loss: 0.490741, accuracy: 82.09%\n",
            "step:  820 | loss: 0.490535, accuracy: 82.10%\n",
            "step:  821 | loss: 0.490391, accuracy: 82.10%\n",
            "step:  822 | loss: 0.489868, accuracy: 82.12%\n",
            "step:  823 | loss: 0.489531, accuracy: 82.13%\n",
            "step:  824 | loss: 0.489192, accuracy: 82.15%\n",
            "step:  825 | loss: 0.489035, accuracy: 82.15%\n",
            "step:  826 | loss: 0.488681, accuracy: 82.16%\n",
            "step:  827 | loss: 0.488224, accuracy: 82.18%\n",
            "step:  828 | loss: 0.487821, accuracy: 82.19%\n",
            "step:  829 | loss: 0.487466, accuracy: 82.21%\n",
            "step:  830 | loss: 0.487240, accuracy: 82.22%\n",
            "step:  831 | loss: 0.486817, accuracy: 82.23%\n",
            "step:  832 | loss: 0.486381, accuracy: 82.25%\n",
            "step:  833 | loss: 0.486072, accuracy: 82.26%\n",
            "step:  834 | loss: 0.485656, accuracy: 82.28%\n",
            "step:  835 | loss: 0.485304, accuracy: 82.29%\n",
            "step:  836 | loss: 0.484805, accuracy: 82.31%\n",
            "step:  837 | loss: 0.484440, accuracy: 82.33%\n",
            "step:  838 | loss: 0.484426, accuracy: 82.33%\n",
            "step:  839 | loss: 0.484264, accuracy: 82.33%\n",
            "step:  840 | loss: 0.484136, accuracy: 82.35%\n",
            "step:  841 | loss: 0.483712, accuracy: 82.35%\n",
            "step:  842 | loss: 0.483660, accuracy: 82.36%\n",
            "step:  843 | loss: 0.483204, accuracy: 82.38%\n",
            "step:  844 | loss: 0.482898, accuracy: 82.39%\n",
            "step:  845 | loss: 0.482409, accuracy: 82.41%\n",
            "step:  846 | loss: 0.482218, accuracy: 82.41%\n",
            "step:  847 | loss: 0.481749, accuracy: 82.43%\n",
            "step:  848 | loss: 0.481685, accuracy: 82.43%\n",
            "step:  849 | loss: 0.481512, accuracy: 82.44%\n",
            "step:  850 | loss: 0.481210, accuracy: 82.45%\n",
            "step:  851 | loss: 0.481021, accuracy: 82.46%\n",
            "step:  852 | loss: 0.480604, accuracy: 82.47%\n",
            "step:  853 | loss: 0.480449, accuracy: 82.49%\n",
            "step:  854 | loss: 0.479919, accuracy: 82.51%\n",
            "step:  855 | loss: 0.480429, accuracy: 82.50%\n",
            "step:  856 | loss: 0.480046, accuracy: 82.52%\n",
            "step:  857 | loss: 0.479522, accuracy: 82.54%\n",
            "step:  858 | loss: 0.479085, accuracy: 82.55%\n",
            "step:  859 | loss: 0.479004, accuracy: 82.56%\n",
            "step:  860 | loss: 0.478679, accuracy: 82.57%\n",
            "step:  861 | loss: 0.478234, accuracy: 82.58%\n",
            "step:  862 | loss: 0.477999, accuracy: 82.59%\n",
            "step:  863 | loss: 0.477627, accuracy: 82.60%\n",
            "step:  864 | loss: 0.477273, accuracy: 82.62%\n",
            "step:  865 | loss: 0.476772, accuracy: 82.64%\n",
            "step:  866 | loss: 0.476543, accuracy: 82.64%\n",
            "step:  867 | loss: 0.476200, accuracy: 82.65%\n",
            "step:  868 | loss: 0.475828, accuracy: 82.67%\n",
            "step:  869 | loss: 0.475753, accuracy: 82.68%\n",
            "step:  870 | loss: 0.475422, accuracy: 82.68%\n",
            "step:  871 | loss: 0.475128, accuracy: 82.69%\n",
            "step:  872 | loss: 0.474715, accuracy: 82.71%\n",
            "step:  873 | loss: 0.474827, accuracy: 82.71%\n",
            "step:  874 | loss: 0.474745, accuracy: 82.72%\n",
            "step:  875 | loss: 0.474565, accuracy: 82.73%\n",
            "step:  876 | loss: 0.474110, accuracy: 82.75%\n",
            "step:  877 | loss: 0.473719, accuracy: 82.77%\n",
            "step:  878 | loss: 0.473448, accuracy: 82.77%\n",
            "step:  879 | loss: 0.472936, accuracy: 82.79%\n",
            "step:  880 | loss: 0.472652, accuracy: 82.80%\n",
            "step:  881 | loss: 0.472297, accuracy: 82.80%\n",
            "step:  882 | loss: 0.471914, accuracy: 82.81%\n",
            "step:  883 | loss: 0.471577, accuracy: 82.83%\n",
            "step:  884 | loss: 0.471241, accuracy: 82.84%\n",
            "step:  885 | loss: 0.470934, accuracy: 82.84%\n",
            "step:  886 | loss: 0.470763, accuracy: 82.84%\n",
            "step:  887 | loss: 0.470315, accuracy: 82.85%\n",
            "step:  888 | loss: 0.469957, accuracy: 82.86%\n",
            "step:  889 | loss: 0.469722, accuracy: 82.87%\n",
            "step:  890 | loss: 0.469301, accuracy: 82.88%\n",
            "step:  891 | loss: 0.469049, accuracy: 82.90%\n",
            "step:  892 | loss: 0.469179, accuracy: 82.90%\n",
            "step:  893 | loss: 0.468959, accuracy: 82.91%\n",
            "step:  894 | loss: 0.468577, accuracy: 82.92%\n",
            "step:  895 | loss: 0.468187, accuracy: 82.93%\n",
            "step:  896 | loss: 0.467824, accuracy: 82.94%\n",
            "step:  897 | loss: 0.467640, accuracy: 82.95%\n",
            "step:  898 | loss: 0.467471, accuracy: 82.96%\n",
            "step:  899 | loss: 0.467033, accuracy: 82.98%\n",
            "step:  900 | loss: 0.466691, accuracy: 82.99%\n",
            "step:  901 | loss: 0.466772, accuracy: 82.98%\n",
            "step:  902 | loss: 0.466663, accuracy: 82.99%\n",
            "step:  903 | loss: 0.466215, accuracy: 83.01%\n",
            "step:  904 | loss: 0.465933, accuracy: 83.02%\n",
            "step:  905 | loss: 0.465464, accuracy: 83.04%\n",
            "step:  906 | loss: 0.465072, accuracy: 83.05%\n",
            "step:  907 | loss: 0.464607, accuracy: 83.07%\n",
            "step:  908 | loss: 0.464203, accuracy: 83.08%\n",
            "step:  909 | loss: 0.463731, accuracy: 83.10%\n",
            "step:  910 | loss: 0.463291, accuracy: 83.12%\n",
            "step:  911 | loss: 0.463053, accuracy: 83.13%\n",
            "step:  912 | loss: 0.462813, accuracy: 83.14%\n",
            "step:  913 | loss: 0.462431, accuracy: 83.15%\n",
            "step:  914 | loss: 0.462089, accuracy: 83.16%\n",
            "step:  915 | loss: 0.461775, accuracy: 83.17%\n",
            "step:  916 | loss: 0.461420, accuracy: 83.18%\n",
            "step:  917 | loss: 0.460976, accuracy: 83.20%\n",
            "step:  918 | loss: 0.460610, accuracy: 83.21%\n",
            "step:  919 | loss: 0.460296, accuracy: 83.22%\n",
            "step:  920 | loss: 0.460341, accuracy: 83.22%\n",
            "step:  921 | loss: 0.460151, accuracy: 83.22%\n",
            "step:  922 | loss: 0.459914, accuracy: 83.23%\n",
            "step:  923 | loss: 0.459537, accuracy: 83.24%\n",
            "step:  924 | loss: 0.459080, accuracy: 83.26%\n",
            "step:  925 | loss: 0.458838, accuracy: 83.27%\n",
            "step:  926 | loss: 0.458665, accuracy: 83.28%\n",
            "step:  927 | loss: 0.458279, accuracy: 83.29%\n",
            "step:  928 | loss: 0.457894, accuracy: 83.30%\n",
            "step:  929 | loss: 0.457556, accuracy: 83.31%\n",
            "step:  930 | loss: 0.457223, accuracy: 83.32%\n",
            "step:  931 | loss: 0.456846, accuracy: 83.34%\n",
            "step:  932 | loss: 0.456615, accuracy: 83.34%\n",
            "step:  933 | loss: 0.456247, accuracy: 83.36%\n",
            "step:  934 | loss: 0.455907, accuracy: 83.37%\n",
            "step:  935 | loss: 0.455703, accuracy: 83.39%\n",
            "step:  936 | loss: 0.455323, accuracy: 83.40%\n",
            "step:  937 | loss: 0.454905, accuracy: 83.42%\n",
            "step:  938 | loss: 0.454509, accuracy: 83.43%\n",
            "step:  939 | loss: 0.454105, accuracy: 83.45%\n",
            "step:  940 | loss: 0.454068, accuracy: 83.45%\n",
            "step:  941 | loss: 0.453736, accuracy: 83.46%\n",
            "step:  942 | loss: 0.453591, accuracy: 83.46%\n",
            "step:  943 | loss: 0.453116, accuracy: 83.47%\n",
            "step:  944 | loss: 0.452845, accuracy: 83.48%\n",
            "step:  945 | loss: 0.452449, accuracy: 83.50%\n",
            "step:  946 | loss: 0.452205, accuracy: 83.51%\n",
            "step:  947 | loss: 0.451869, accuracy: 83.52%\n",
            "step:  948 | loss: 0.451577, accuracy: 83.53%\n",
            "step:  949 | loss: 0.451185, accuracy: 83.55%\n",
            "step:  950 | loss: 0.450960, accuracy: 83.55%\n",
            "step:  951 | loss: 0.450812, accuracy: 83.56%\n",
            "step:  952 | loss: 0.450620, accuracy: 83.57%\n",
            "step:  953 | loss: 0.450414, accuracy: 83.57%\n",
            "step:  954 | loss: 0.450012, accuracy: 83.59%\n",
            "step:  955 | loss: 0.449652, accuracy: 83.60%\n",
            "step:  956 | loss: 0.449246, accuracy: 83.61%\n",
            "step:  957 | loss: 0.449065, accuracy: 83.63%\n",
            "step:  958 | loss: 0.448863, accuracy: 83.63%\n",
            "step:  959 | loss: 0.448491, accuracy: 83.64%\n",
            "step:  960 | loss: 0.448296, accuracy: 83.66%\n",
            "step:  961 | loss: 0.448036, accuracy: 83.66%\n",
            "step:  962 | loss: 0.447701, accuracy: 83.67%\n",
            "step:  963 | loss: 0.447272, accuracy: 83.69%\n",
            "step:  964 | loss: 0.447100, accuracy: 83.70%\n",
            "step:  965 | loss: 0.446656, accuracy: 83.72%\n",
            "step:  966 | loss: 0.446486, accuracy: 83.72%\n",
            "step:  967 | loss: 0.446252, accuracy: 83.72%\n",
            "step:  968 | loss: 0.445990, accuracy: 83.73%\n",
            "step:  969 | loss: 0.445660, accuracy: 83.75%\n",
            "step:  970 | loss: 0.445459, accuracy: 83.75%\n",
            "step:  971 | loss: 0.445075, accuracy: 83.77%\n",
            "step:  972 | loss: 0.444877, accuracy: 83.78%\n",
            "step:  973 | loss: 0.444619, accuracy: 83.78%\n",
            "step:  974 | loss: 0.444171, accuracy: 83.80%\n",
            "step:  975 | loss: 0.444028, accuracy: 83.79%\n",
            "step:  976 | loss: 0.443628, accuracy: 83.81%\n",
            "step:  977 | loss: 0.443580, accuracy: 83.82%\n",
            "step:  978 | loss: 0.443500, accuracy: 83.81%\n",
            "step:  979 | loss: 0.443527, accuracy: 83.82%\n",
            "step:  980 | loss: 0.443221, accuracy: 83.83%\n",
            "step:  981 | loss: 0.443000, accuracy: 83.84%\n",
            "step:  982 | loss: 0.442943, accuracy: 83.84%\n",
            "step:  983 | loss: 0.442520, accuracy: 83.86%\n",
            "step:  984 | loss: 0.442270, accuracy: 83.87%\n",
            "step:  985 | loss: 0.441918, accuracy: 83.88%\n",
            "step:  986 | loss: 0.441779, accuracy: 83.89%\n",
            "step:  987 | loss: 0.441758, accuracy: 83.90%\n",
            "step:  988 | loss: 0.441647, accuracy: 83.90%\n",
            "step:  989 | loss: 0.441442, accuracy: 83.90%\n",
            "step:  990 | loss: 0.441109, accuracy: 83.92%\n",
            "step:  991 | loss: 0.441090, accuracy: 83.92%\n",
            "step:  992 | loss: 0.440908, accuracy: 83.92%\n",
            "step:  993 | loss: 0.440720, accuracy: 83.93%\n",
            "step:  994 | loss: 0.440723, accuracy: 83.93%\n",
            "step:  995 | loss: 0.440293, accuracy: 83.94%\n",
            "step:  996 | loss: 0.440061, accuracy: 83.95%\n",
            "step:  997 | loss: 0.439867, accuracy: 83.96%\n",
            "step:  998 | loss: 0.439810, accuracy: 83.96%\n",
            "step:  999 | loss: 0.439412, accuracy: 83.98%\n",
            "step: 1000 | loss: 0.439054, accuracy: 83.99%\n",
            "\n",
            "Use val dataset\n",
            "[EVAL] step:    1 | accuracy: 95.00%\n",
            "[EVAL] step:    2 | accuracy: 87.50%\n",
            "[EVAL] step:    3 | accuracy: 88.33%\n",
            "[EVAL] step:    4 | accuracy: 86.25%\n",
            "[EVAL] step:    5 | accuracy: 87.00%\n",
            "[EVAL] step:    6 | accuracy: 85.83%\n",
            "[EVAL] step:    7 | accuracy: 84.29%\n",
            "[EVAL] step:    8 | accuracy: 84.38%\n",
            "[EVAL] step:    9 | accuracy: 82.22%\n",
            "[EVAL] step:   10 | accuracy: 83.50%\n",
            "[EVAL] step:   11 | accuracy: 83.64%\n",
            "[EVAL] step:   12 | accuracy: 82.50%\n",
            "[EVAL] step:   13 | accuracy: 81.54%\n",
            "[EVAL] step:   14 | accuracy: 80.36%\n",
            "[EVAL] step:   15 | accuracy: 81.00%\n",
            "[EVAL] step:   16 | accuracy: 81.25%\n",
            "[EVAL] step:   17 | accuracy: 80.59%\n",
            "[EVAL] step:   18 | accuracy: 80.56%\n",
            "[EVAL] step:   19 | accuracy: 80.79%\n",
            "[EVAL] step:   20 | accuracy: 80.25%\n",
            "[EVAL] step:   21 | accuracy: 80.48%\n",
            "[EVAL] step:   22 | accuracy: 80.45%\n",
            "[EVAL] step:   23 | accuracy: 80.43%\n",
            "[EVAL] step:   24 | accuracy: 80.21%\n",
            "[EVAL] step:   25 | accuracy: 79.80%\n",
            "[EVAL] step:   26 | accuracy: 79.62%\n",
            "[EVAL] step:   27 | accuracy: 79.81%\n",
            "[EVAL] step:   28 | accuracy: 79.64%\n",
            "[EVAL] step:   29 | accuracy: 79.66%\n",
            "[EVAL] step:   30 | accuracy: 79.67%\n",
            "[EVAL] step:   31 | accuracy: 79.68%\n",
            "[EVAL] step:   32 | accuracy: 79.69%\n",
            "[EVAL] step:   33 | accuracy: 79.85%\n",
            "[EVAL] step:   34 | accuracy: 80.00%\n",
            "[EVAL] step:   35 | accuracy: 80.43%\n",
            "[EVAL] step:   36 | accuracy: 80.14%\n",
            "[EVAL] step:   37 | accuracy: 80.14%\n",
            "[EVAL] step:   38 | accuracy: 80.26%\n",
            "[EVAL] step:   39 | accuracy: 80.38%\n",
            "[EVAL] step:   40 | accuracy: 80.38%\n",
            "[EVAL] step:   41 | accuracy: 80.73%\n",
            "[EVAL] step:   42 | accuracy: 80.95%\n",
            "[EVAL] step:   43 | accuracy: 81.05%\n",
            "[EVAL] step:   44 | accuracy: 81.25%\n",
            "[EVAL] step:   45 | accuracy: 81.22%\n",
            "[EVAL] step:   46 | accuracy: 81.09%\n",
            "[EVAL] step:   47 | accuracy: 81.17%\n",
            "[EVAL] step:   48 | accuracy: 80.94%\n",
            "[EVAL] step:   49 | accuracy: 80.41%\n",
            "[EVAL] step:   50 | accuracy: 80.30%\n",
            "[EVAL] step:   51 | accuracy: 80.20%\n",
            "[EVAL] step:   52 | accuracy: 80.19%\n",
            "[EVAL] step:   53 | accuracy: 80.47%\n",
            "[EVAL] step:   54 | accuracy: 80.65%\n",
            "[EVAL] step:   55 | accuracy: 80.64%\n",
            "[EVAL] step:   56 | accuracy: 80.27%\n",
            "[EVAL] step:   57 | accuracy: 80.35%\n",
            "[EVAL] step:   58 | accuracy: 80.26%\n",
            "[EVAL] step:   59 | accuracy: 80.17%\n",
            "[EVAL] step:   60 | accuracy: 80.08%\n",
            "[EVAL] step:   61 | accuracy: 80.25%\n",
            "[EVAL] step:   62 | accuracy: 80.24%\n",
            "[EVAL] step:   63 | accuracy: 80.24%\n",
            "[EVAL] step:   64 | accuracy: 80.39%\n",
            "[EVAL] step:   65 | accuracy: 80.46%\n",
            "[EVAL] step:   66 | accuracy: 80.38%\n",
            "[EVAL] step:   67 | accuracy: 80.52%\n",
            "[EVAL] step:   68 | accuracy: 80.37%\n",
            "[EVAL] step:   69 | accuracy: 80.51%\n",
            "[EVAL] step:   70 | accuracy: 80.64%\n",
            "[EVAL] step:   71 | accuracy: 80.70%\n",
            "[EVAL] step:   72 | accuracy: 80.63%\n",
            "[EVAL] step:   73 | accuracy: 80.41%\n",
            "[EVAL] step:   74 | accuracy: 80.54%\n",
            "[EVAL] step:   75 | accuracy: 80.60%\n",
            "[EVAL] step:   76 | accuracy: 80.59%\n",
            "[EVAL] step:   77 | accuracy: 80.58%\n",
            "[EVAL] step:   78 | accuracy: 80.58%\n",
            "[EVAL] step:   79 | accuracy: 80.57%\n",
            "[EVAL] step:   80 | accuracy: 80.69%\n",
            "[EVAL] step:   81 | accuracy: 80.68%\n",
            "[EVAL] step:   82 | accuracy: 80.73%\n",
            "[EVAL] step:   83 | accuracy: 80.60%\n",
            "[EVAL] step:   84 | accuracy: 80.60%\n",
            "[EVAL] step:   85 | accuracy: 80.65%\n",
            "[EVAL] step:   86 | accuracy: 80.76%\n",
            "[EVAL] step:   87 | accuracy: 80.80%\n",
            "[EVAL] step:   88 | accuracy: 80.80%\n",
            "[EVAL] step:   89 | accuracy: 80.79%\n",
            "[EVAL] step:   90 | accuracy: 80.78%\n",
            "[EVAL] step:   91 | accuracy: 80.49%\n",
            "[EVAL] step:   92 | accuracy: 80.33%\n",
            "[EVAL] step:   93 | accuracy: 80.43%\n",
            "[EVAL] step:   94 | accuracy: 80.59%\n",
            "[EVAL] step:   95 | accuracy: 80.63%\n",
            "[EVAL] step:   96 | accuracy: 80.78%\n",
            "[EVAL] step:   97 | accuracy: 80.82%\n",
            "[EVAL] step:   98 | accuracy: 80.87%\n",
            "[EVAL] step:   99 | accuracy: 80.81%\n",
            "[EVAL] step:  100 | accuracy: 80.80%\n",
            "[EVAL] step:  101 | accuracy: 80.74%\n",
            "[EVAL] step:  102 | accuracy: 80.78%\n",
            "[EVAL] step:  103 | accuracy: 80.73%\n",
            "[EVAL] step:  104 | accuracy: 80.82%\n",
            "[EVAL] step:  105 | accuracy: 80.90%\n",
            "[EVAL] step:  106 | accuracy: 81.08%\n",
            "[EVAL] step:  107 | accuracy: 81.21%\n",
            "[EVAL] step:  108 | accuracy: 81.30%\n",
            "[EVAL] step:  109 | accuracy: 81.28%\n",
            "[EVAL] step:  110 | accuracy: 81.27%\n",
            "[EVAL] step:  111 | accuracy: 81.35%\n",
            "[EVAL] step:  112 | accuracy: 81.34%\n",
            "[EVAL] step:  113 | accuracy: 81.37%\n",
            "[EVAL] step:  114 | accuracy: 81.40%\n",
            "[EVAL] step:  115 | accuracy: 81.35%\n",
            "[EVAL] step:  116 | accuracy: 81.47%\n",
            "[EVAL] step:  117 | accuracy: 81.58%\n",
            "[EVAL] step:  118 | accuracy: 81.65%\n",
            "[EVAL] step:  119 | accuracy: 81.68%\n",
            "[EVAL] step:  120 | accuracy: 81.63%\n",
            "[EVAL] step:  121 | accuracy: 81.57%\n",
            "[EVAL] step:  122 | accuracy: 81.60%\n",
            "[EVAL] step:  123 | accuracy: 81.67%\n",
            "[EVAL] step:  124 | accuracy: 81.61%\n",
            "[EVAL] step:  125 | accuracy: 81.64%\n",
            "[EVAL] step:  126 | accuracy: 81.67%\n",
            "[EVAL] step:  127 | accuracy: 81.65%\n",
            "[EVAL] step:  128 | accuracy: 81.68%\n",
            "[EVAL] step:  129 | accuracy: 81.63%\n",
            "[EVAL] step:  130 | accuracy: 81.73%\n",
            "[EVAL] step:  131 | accuracy: 81.83%\n",
            "[EVAL] step:  132 | accuracy: 81.70%\n",
            "[EVAL] step:  133 | accuracy: 81.77%\n",
            "[EVAL] step:  134 | accuracy: 81.75%\n",
            "[EVAL] step:  135 | accuracy: 81.78%\n",
            "[EVAL] step:  136 | accuracy: 81.80%\n",
            "[EVAL] step:  137 | accuracy: 81.86%\n",
            "[EVAL] step:  138 | accuracy: 81.81%\n",
            "[EVAL] step:  139 | accuracy: 81.87%\n",
            "[EVAL] step:  140 | accuracy: 81.86%\n",
            "[EVAL] step:  141 | accuracy: 81.88%\n",
            "[EVAL] step:  142 | accuracy: 81.90%\n",
            "[EVAL] step:  143 | accuracy: 81.96%\n",
            "[EVAL] step:  144 | accuracy: 82.05%\n",
            "[EVAL] step:  145 | accuracy: 82.03%\n",
            "[EVAL] step:  146 | accuracy: 82.05%\n",
            "[EVAL] step:  147 | accuracy: 82.11%\n",
            "[EVAL] step:  148 | accuracy: 82.16%\n",
            "[EVAL] step:  149 | accuracy: 82.21%\n",
            "[EVAL] step:  150 | accuracy: 82.13%\n",
            "[EVAL] step:  151 | accuracy: 82.22%\n",
            "[EVAL] step:  152 | accuracy: 82.34%\n",
            "[EVAL] step:  153 | accuracy: 82.35%\n",
            "[EVAL] step:  154 | accuracy: 82.44%\n",
            "[EVAL] step:  155 | accuracy: 82.52%\n",
            "[EVAL] step:  156 | accuracy: 82.56%\n",
            "[EVAL] step:  157 | accuracy: 82.68%\n",
            "[EVAL] step:  158 | accuracy: 82.72%\n",
            "[EVAL] step:  159 | accuracy: 82.64%\n",
            "[EVAL] step:  160 | accuracy: 82.66%\n",
            "[EVAL] step:  161 | accuracy: 82.73%\n",
            "[EVAL] step:  162 | accuracy: 82.69%\n",
            "[EVAL] step:  163 | accuracy: 82.70%\n",
            "[EVAL] step:  164 | accuracy: 82.74%\n",
            "[EVAL] step:  165 | accuracy: 82.76%\n",
            "[EVAL] step:  166 | accuracy: 82.77%\n",
            "[EVAL] step:  167 | accuracy: 82.75%\n",
            "[EVAL] step:  168 | accuracy: 82.74%\n",
            "[EVAL] step:  169 | accuracy: 82.72%\n",
            "[EVAL] step:  170 | accuracy: 82.68%\n",
            "[EVAL] step:  171 | accuracy: 82.66%\n",
            "[EVAL] step:  172 | accuracy: 82.67%\n",
            "[EVAL] step:  173 | accuracy: 82.69%\n",
            "[EVAL] step:  174 | accuracy: 82.70%\n",
            "[EVAL] step:  175 | accuracy: 82.77%\n",
            "[EVAL] step:  176 | accuracy: 82.76%\n",
            "[EVAL] step:  177 | accuracy: 82.71%\n",
            "[EVAL] step:  178 | accuracy: 82.61%\n",
            "[EVAL] step:  179 | accuracy: 82.71%\n",
            "[EVAL] step:  180 | accuracy: 82.75%\n",
            "[EVAL] step:  181 | accuracy: 82.79%\n",
            "[EVAL] step:  182 | accuracy: 82.75%\n",
            "[EVAL] step:  183 | accuracy: 82.73%\n",
            "[EVAL] step:  184 | accuracy: 82.69%\n",
            "[EVAL] step:  185 | accuracy: 82.68%\n",
            "[EVAL] step:  186 | accuracy: 82.63%\n",
            "[EVAL] step:  187 | accuracy: 82.62%\n",
            "[EVAL] step:  188 | accuracy: 82.66%\n",
            "[EVAL] step:  189 | accuracy: 82.67%\n",
            "[EVAL] step:  190 | accuracy: 82.61%\n",
            "[EVAL] step:  191 | accuracy: 82.64%\n",
            "[EVAL] step:  192 | accuracy: 82.63%\n",
            "[EVAL] step:  193 | accuracy: 82.64%\n",
            "[EVAL] step:  194 | accuracy: 82.68%\n",
            "[EVAL] step:  195 | accuracy: 82.67%\n",
            "[EVAL] step:  196 | accuracy: 82.68%\n",
            "[EVAL] step:  197 | accuracy: 82.74%\n",
            "[EVAL] step:  198 | accuracy: 82.68%\n",
            "[EVAL] step:  199 | accuracy: 82.64%\n",
            "[EVAL] step:  200 | accuracy: 82.63%\n",
            "[EVAL] step:  201 | accuracy: 82.59%\n",
            "[EVAL] step:  202 | accuracy: 82.55%\n",
            "[EVAL] step:  203 | accuracy: 82.59%\n",
            "[EVAL] step:  204 | accuracy: 82.62%\n",
            "[EVAL] step:  205 | accuracy: 82.68%\n",
            "[EVAL] step:  206 | accuracy: 82.65%\n",
            "[EVAL] step:  207 | accuracy: 82.56%\n",
            "[EVAL] step:  208 | accuracy: 82.55%\n",
            "[EVAL] step:  209 | accuracy: 82.51%\n",
            "[EVAL] step:  210 | accuracy: 82.50%\n",
            "[EVAL] step:  211 | accuracy: 82.54%\n",
            "[EVAL] step:  212 | accuracy: 82.55%\n",
            "[EVAL] step:  213 | accuracy: 82.56%\n",
            "[EVAL] step:  214 | accuracy: 82.62%\n",
            "[EVAL] step:  215 | accuracy: 82.65%\n",
            "[EVAL] step:  216 | accuracy: 82.71%\n",
            "[EVAL] step:  217 | accuracy: 82.72%\n",
            "[EVAL] step:  218 | accuracy: 82.73%\n",
            "[EVAL] step:  219 | accuracy: 82.69%\n",
            "[EVAL] step:  220 | accuracy: 82.70%\n",
            "[EVAL] step:  221 | accuracy: 82.69%\n",
            "[EVAL] step:  222 | accuracy: 82.70%\n",
            "[EVAL] step:  223 | accuracy: 82.69%\n",
            "[EVAL] step:  224 | accuracy: 82.68%\n",
            "[EVAL] step:  225 | accuracy: 82.67%\n",
            "[EVAL] step:  226 | accuracy: 82.74%\n",
            "[EVAL] step:  227 | accuracy: 82.67%\n",
            "[EVAL] step:  228 | accuracy: 82.63%\n",
            "[EVAL] step:  229 | accuracy: 82.62%\n",
            "[EVAL] step:  230 | accuracy: 82.61%\n",
            "[EVAL] step:  231 | accuracy: 82.64%\n",
            "[EVAL] step:  232 | accuracy: 82.54%\n",
            "[EVAL] step:  233 | accuracy: 82.60%\n",
            "[EVAL] step:  234 | accuracy: 82.56%\n",
            "[EVAL] step:  235 | accuracy: 82.57%\n",
            "[EVAL] step:  236 | accuracy: 82.52%\n",
            "[EVAL] step:  237 | accuracy: 82.57%\n",
            "[EVAL] step:  238 | accuracy: 82.65%\n",
            "[EVAL] step:  239 | accuracy: 82.66%\n",
            "[EVAL] step:  240 | accuracy: 82.69%\n",
            "[EVAL] step:  241 | accuracy: 82.74%\n",
            "[EVAL] step:  242 | accuracy: 82.77%\n",
            "[EVAL] step:  243 | accuracy: 82.76%\n",
            "[EVAL] step:  244 | accuracy: 82.77%\n",
            "[EVAL] step:  245 | accuracy: 82.78%\n",
            "[EVAL] step:  246 | accuracy: 82.76%\n",
            "[EVAL] step:  247 | accuracy: 82.77%\n",
            "[EVAL] step:  248 | accuracy: 82.84%\n",
            "[EVAL] step:  249 | accuracy: 82.89%\n",
            "[EVAL] step:  250 | accuracy: 82.88%\n",
            "[EVAL] step:  251 | accuracy: 82.87%\n",
            "[EVAL] step:  252 | accuracy: 82.86%\n",
            "[EVAL] step:  253 | accuracy: 82.87%\n",
            "[EVAL] step:  254 | accuracy: 82.83%\n",
            "[EVAL] step:  255 | accuracy: 82.84%\n",
            "[EVAL] step:  256 | accuracy: 82.87%\n",
            "[EVAL] step:  257 | accuracy: 82.82%\n",
            "[EVAL] step:  258 | accuracy: 82.85%\n",
            "[EVAL] step:  259 | accuracy: 82.80%\n",
            "[EVAL] step:  260 | accuracy: 82.77%\n",
            "[EVAL] step:  261 | accuracy: 82.78%\n",
            "[EVAL] step:  262 | accuracy: 82.79%\n",
            "[EVAL] step:  263 | accuracy: 82.79%\n",
            "[EVAL] step:  264 | accuracy: 82.77%\n",
            "[EVAL] step:  265 | accuracy: 82.72%\n",
            "[EVAL] step:  266 | accuracy: 82.73%\n",
            "[EVAL] step:  267 | accuracy: 82.73%\n",
            "[EVAL] step:  268 | accuracy: 82.76%\n",
            "[EVAL] step:  269 | accuracy: 82.75%\n",
            "[EVAL] step:  270 | accuracy: 82.76%\n",
            "[EVAL] step:  271 | accuracy: 82.80%\n",
            "[EVAL] step:  272 | accuracy: 82.78%\n",
            "[EVAL] step:  273 | accuracy: 82.82%\n",
            "[EVAL] step:  274 | accuracy: 82.85%\n",
            "[EVAL] step:  275 | accuracy: 82.84%\n",
            "[EVAL] step:  276 | accuracy: 82.83%\n",
            "[EVAL] step:  277 | accuracy: 82.85%\n",
            "[EVAL] step:  278 | accuracy: 82.90%\n",
            "[EVAL] step:  279 | accuracy: 82.89%\n",
            "[EVAL] step:  280 | accuracy: 82.89%\n",
            "[EVAL] step:  281 | accuracy: 82.94%\n",
            "[EVAL] step:  282 | accuracy: 82.94%\n",
            "[EVAL] step:  283 | accuracy: 82.93%\n",
            "[EVAL] step:  284 | accuracy: 82.92%\n",
            "[EVAL] step:  285 | accuracy: 82.95%\n",
            "[EVAL] step:  286 | accuracy: 82.88%\n",
            "[EVAL] step:  287 | accuracy: 82.87%\n",
            "[EVAL] step:  288 | accuracy: 82.90%\n",
            "[EVAL] step:  289 | accuracy: 82.91%\n",
            "[EVAL] step:  290 | accuracy: 82.95%\n",
            "[EVAL] step:  291 | accuracy: 82.96%\n",
            "[EVAL] step:  292 | accuracy: 83.00%\n",
            "[EVAL] step:  293 | accuracy: 82.99%\n",
            "[EVAL] step:  294 | accuracy: 82.93%\n",
            "[EVAL] step:  295 | accuracy: 82.85%\n",
            "[EVAL] step:  296 | accuracy: 82.84%\n",
            "[EVAL] step:  297 | accuracy: 82.86%\n",
            "[EVAL] step:  298 | accuracy: 82.87%\n",
            "[EVAL] step:  299 | accuracy: 82.88%\n",
            "[EVAL] step:  300 | accuracy: 82.88%\n",
            "[EVAL] step:  301 | accuracy: 82.91%\n",
            "[EVAL] step:  302 | accuracy: 82.88%\n",
            "[EVAL] step:  303 | accuracy: 82.94%\n",
            "[EVAL] step:  304 | accuracy: 82.89%\n",
            "[EVAL] step:  305 | accuracy: 82.89%\n",
            "[EVAL] step:  306 | accuracy: 82.84%\n",
            "[EVAL] step:  307 | accuracy: 82.85%\n",
            "[EVAL] step:  308 | accuracy: 82.89%\n",
            "[EVAL] step:  309 | accuracy: 82.88%\n",
            "[EVAL] step:  310 | accuracy: 82.90%\n",
            "[EVAL] step:  311 | accuracy: 82.91%\n",
            "[EVAL] step:  312 | accuracy: 82.87%\n",
            "[EVAL] step:  313 | accuracy: 82.89%\n",
            "[EVAL] step:  314 | accuracy: 82.88%\n",
            "[EVAL] step:  315 | accuracy: 82.87%\n",
            "[EVAL] step:  316 | accuracy: 82.91%\n",
            "[EVAL] step:  317 | accuracy: 82.97%\n",
            "[EVAL] step:  318 | accuracy: 82.97%\n",
            "[EVAL] step:  319 | accuracy: 82.96%\n",
            "[EVAL] step:  320 | accuracy: 82.94%\n",
            "[EVAL] step:  321 | accuracy: 82.98%\n",
            "[EVAL] step:  322 | accuracy: 82.98%\n",
            "[EVAL] step:  323 | accuracy: 82.96%\n",
            "[EVAL] step:  324 | accuracy: 82.96%\n",
            "[EVAL] step:  325 | accuracy: 82.97%\n",
            "[EVAL] step:  326 | accuracy: 82.98%\n",
            "[EVAL] step:  327 | accuracy: 82.97%\n",
            "[EVAL] step:  328 | accuracy: 82.96%\n",
            "[EVAL] step:  329 | accuracy: 82.95%\n",
            "[EVAL] step:  330 | accuracy: 82.95%\n",
            "[EVAL] step:  331 | accuracy: 82.99%\n",
            "[EVAL] step:  332 | accuracy: 82.97%\n",
            "[EVAL] step:  333 | accuracy: 82.93%\n",
            "[EVAL] step:  334 | accuracy: 82.92%\n",
            "[EVAL] step:  335 | accuracy: 82.93%\n",
            "[EVAL] step:  336 | accuracy: 82.89%\n",
            "[EVAL] step:  337 | accuracy: 82.86%\n",
            "[EVAL] step:  338 | accuracy: 82.83%\n",
            "[EVAL] step:  339 | accuracy: 82.85%\n",
            "[EVAL] step:  340 | accuracy: 82.82%\n",
            "[EVAL] step:  341 | accuracy: 82.82%\n",
            "[EVAL] step:  342 | accuracy: 82.81%\n",
            "[EVAL] step:  343 | accuracy: 82.76%\n",
            "[EVAL] step:  344 | accuracy: 82.75%\n",
            "[EVAL] step:  345 | accuracy: 82.78%\n",
            "[EVAL] step:  346 | accuracy: 82.76%\n",
            "[EVAL] step:  347 | accuracy: 82.80%\n",
            "[EVAL] step:  348 | accuracy: 82.82%\n",
            "[EVAL] step:  349 | accuracy: 82.84%\n",
            "[EVAL] step:  350 | accuracy: 82.81%\n",
            "[EVAL] step:  351 | accuracy: 82.81%\n",
            "[EVAL] step:  352 | accuracy: 82.83%\n",
            "[EVAL] step:  353 | accuracy: 82.80%\n",
            "[EVAL] step:  354 | accuracy: 82.80%\n",
            "[EVAL] step:  355 | accuracy: 82.83%\n",
            "[EVAL] step:  356 | accuracy: 82.82%\n",
            "[EVAL] step:  357 | accuracy: 82.80%\n",
            "[EVAL] step:  358 | accuracy: 82.79%\n",
            "[EVAL] step:  359 | accuracy: 82.74%\n",
            "[EVAL] step:  360 | accuracy: 82.74%\n",
            "[EVAL] step:  361 | accuracy: 82.74%\n",
            "[EVAL] step:  362 | accuracy: 82.75%\n",
            "[EVAL] step:  363 | accuracy: 82.75%\n",
            "[EVAL] step:  364 | accuracy: 82.79%\n",
            "[EVAL] step:  365 | accuracy: 82.75%\n",
            "[EVAL] step:  366 | accuracy: 82.77%\n",
            "[EVAL] step:  367 | accuracy: 82.72%\n",
            "[EVAL] step:  368 | accuracy: 82.70%\n",
            "[EVAL] step:  369 | accuracy: 82.72%\n",
            "[EVAL] step:  370 | accuracy: 82.73%\n",
            "[EVAL] step:  371 | accuracy: 82.74%\n",
            "[EVAL] step:  372 | accuracy: 82.72%\n",
            "[EVAL] step:  373 | accuracy: 82.71%\n",
            "[EVAL] step:  374 | accuracy: 82.73%\n",
            "[EVAL] step:  375 | accuracy: 82.73%\n",
            "[EVAL] step:  376 | accuracy: 82.75%\n",
            "[EVAL] step:  377 | accuracy: 82.76%\n",
            "[EVAL] step:  378 | accuracy: 82.71%\n",
            "[EVAL] step:  379 | accuracy: 82.70%\n",
            "[EVAL] step:  380 | accuracy: 82.72%\n",
            "[EVAL] step:  381 | accuracy: 82.70%\n",
            "[EVAL] step:  382 | accuracy: 82.68%\n",
            "[EVAL] step:  383 | accuracy: 82.66%\n",
            "[EVAL] step:  384 | accuracy: 82.66%\n",
            "[EVAL] step:  385 | accuracy: 82.66%\n",
            "[EVAL] step:  386 | accuracy: 82.66%\n",
            "[EVAL] step:  387 | accuracy: 82.66%\n",
            "[EVAL] step:  388 | accuracy: 82.67%\n",
            "[EVAL] step:  389 | accuracy: 82.69%\n",
            "[EVAL] step:  390 | accuracy: 82.68%\n",
            "[EVAL] step:  391 | accuracy: 82.67%\n",
            "[EVAL] step:  392 | accuracy: 82.69%\n",
            "[EVAL] step:  393 | accuracy: 82.70%\n",
            "[EVAL] step:  394 | accuracy: 82.69%\n",
            "[EVAL] step:  395 | accuracy: 82.70%\n",
            "[EVAL] step:  396 | accuracy: 82.71%\n",
            "[EVAL] step:  397 | accuracy: 82.70%\n",
            "[EVAL] step:  398 | accuracy: 82.71%\n",
            "[EVAL] step:  399 | accuracy: 82.69%\n",
            "[EVAL] step:  400 | accuracy: 82.70%\n",
            "[EVAL] step:  401 | accuracy: 82.68%\n",
            "[EVAL] step:  402 | accuracy: 82.71%\n",
            "[EVAL] step:  403 | accuracy: 82.72%\n",
            "[EVAL] step:  404 | accuracy: 82.71%\n",
            "[EVAL] step:  405 | accuracy: 82.69%\n",
            "[EVAL] step:  406 | accuracy: 82.68%\n",
            "[EVAL] step:  407 | accuracy: 82.73%\n",
            "[EVAL] step:  408 | accuracy: 82.68%\n",
            "[EVAL] step:  409 | accuracy: 82.69%\n",
            "[EVAL] step:  410 | accuracy: 82.68%\n",
            "[EVAL] step:  411 | accuracy: 82.66%\n",
            "[EVAL] step:  412 | accuracy: 82.65%\n",
            "[EVAL] step:  413 | accuracy: 82.69%\n",
            "[EVAL] step:  414 | accuracy: 82.69%\n",
            "[EVAL] step:  415 | accuracy: 82.70%\n",
            "[EVAL] step:  416 | accuracy: 82.67%\n",
            "[EVAL] step:  417 | accuracy: 82.65%\n",
            "[EVAL] step:  418 | accuracy: 82.66%\n",
            "[EVAL] step:  419 | accuracy: 82.66%\n",
            "[EVAL] step:  420 | accuracy: 82.68%\n",
            "[EVAL] step:  421 | accuracy: 82.67%\n",
            "[EVAL] step:  422 | accuracy: 82.70%\n",
            "[EVAL] step:  423 | accuracy: 82.72%\n",
            "[EVAL] step:  424 | accuracy: 82.71%\n",
            "[EVAL] step:  425 | accuracy: 82.69%\n",
            "[EVAL] step:  426 | accuracy: 82.66%\n",
            "[EVAL] step:  427 | accuracy: 82.67%\n",
            "[EVAL] step:  428 | accuracy: 82.69%\n",
            "[EVAL] step:  429 | accuracy: 82.69%\n",
            "[EVAL] step:  430 | accuracy: 82.70%\n",
            "[EVAL] step:  431 | accuracy: 82.73%\n",
            "[EVAL] step:  432 | accuracy: 82.71%\n",
            "[EVAL] step:  433 | accuracy: 82.70%\n",
            "[EVAL] step:  434 | accuracy: 82.71%\n",
            "[EVAL] step:  435 | accuracy: 82.71%\n",
            "[EVAL] step:  436 | accuracy: 82.72%\n",
            "[EVAL] step:  437 | accuracy: 82.72%\n",
            "[EVAL] step:  438 | accuracy: 82.72%\n",
            "[EVAL] step:  439 | accuracy: 82.71%\n",
            "[EVAL] step:  440 | accuracy: 82.74%\n",
            "[EVAL] step:  441 | accuracy: 82.73%\n",
            "[EVAL] step:  442 | accuracy: 82.74%\n",
            "[EVAL] step:  443 | accuracy: 82.77%\n",
            "[EVAL] step:  444 | accuracy: 82.79%\n",
            "[EVAL] step:  445 | accuracy: 82.82%\n",
            "[EVAL] step:  446 | accuracy: 82.79%\n",
            "[EVAL] step:  447 | accuracy: 82.81%\n",
            "[EVAL] step:  448 | accuracy: 82.80%\n",
            "[EVAL] step:  449 | accuracy: 82.80%\n",
            "[EVAL] step:  450 | accuracy: 82.79%\n",
            "[EVAL] step:  451 | accuracy: 82.80%\n",
            "[EVAL] step:  452 | accuracy: 82.82%\n",
            "[EVAL] step:  453 | accuracy: 82.83%\n",
            "[EVAL] step:  454 | accuracy: 82.83%\n",
            "[EVAL] step:  455 | accuracy: 82.81%\n",
            "[EVAL] step:  456 | accuracy: 82.83%\n",
            "[EVAL] step:  457 | accuracy: 82.84%\n",
            "[EVAL] step:  458 | accuracy: 82.86%\n",
            "[EVAL] step:  459 | accuracy: 82.86%\n",
            "[EVAL] step:  460 | accuracy: 82.87%\n",
            "[EVAL] step:  461 | accuracy: 82.86%\n",
            "[EVAL] step:  462 | accuracy: 82.85%\n",
            "[EVAL] step:  463 | accuracy: 82.85%\n",
            "[EVAL] step:  464 | accuracy: 82.88%\n",
            "[EVAL] step:  465 | accuracy: 82.89%\n",
            "[EVAL] step:  466 | accuracy: 82.89%\n",
            "[EVAL] step:  467 | accuracy: 82.90%\n",
            "[EVAL] step:  468 | accuracy: 82.90%\n",
            "[EVAL] step:  469 | accuracy: 82.89%\n",
            "[EVAL] step:  470 | accuracy: 82.89%\n",
            "[EVAL] step:  471 | accuracy: 82.87%\n",
            "[EVAL] step:  472 | accuracy: 82.86%\n",
            "[EVAL] step:  473 | accuracy: 82.86%\n",
            "[EVAL] step:  474 | accuracy: 82.87%\n",
            "[EVAL] step:  475 | accuracy: 82.89%\n",
            "[EVAL] step:  476 | accuracy: 82.88%\n",
            "[EVAL] step:  477 | accuracy: 82.86%\n",
            "[EVAL] step:  478 | accuracy: 82.87%\n",
            "[EVAL] step:  479 | accuracy: 82.88%\n",
            "[EVAL] step:  480 | accuracy: 82.90%\n",
            "[EVAL] step:  481 | accuracy: 82.91%\n",
            "[EVAL] step:  482 | accuracy: 82.91%\n",
            "[EVAL] step:  483 | accuracy: 82.91%\n",
            "[EVAL] step:  484 | accuracy: 82.91%\n",
            "[EVAL] step:  485 | accuracy: 82.92%\n",
            "[EVAL] step:  486 | accuracy: 82.92%\n",
            "[EVAL] step:  487 | accuracy: 82.93%\n",
            "[EVAL] step:  488 | accuracy: 82.90%\n",
            "[EVAL] step:  489 | accuracy: 82.88%\n",
            "[EVAL] step:  490 | accuracy: 82.88%\n",
            "[EVAL] step:  491 | accuracy: 82.88%\n",
            "[EVAL] step:  492 | accuracy: 82.86%\n",
            "[EVAL] step:  493 | accuracy: 82.81%\n",
            "[EVAL] step:  494 | accuracy: 82.81%\n",
            "[EVAL] step:  495 | accuracy: 82.82%\n",
            "[EVAL] step:  496 | accuracy: 82.82%\n",
            "[EVAL] step:  497 | accuracy: 82.83%\n",
            "[EVAL] step:  498 | accuracy: 82.85%\n",
            "[EVAL] step:  499 | accuracy: 82.85%\n",
            "[EVAL] step:  500 | accuracy: 82.85%\n",
            "[EVAL] step:  501 | accuracy: 82.86%\n",
            "[EVAL] step:  502 | accuracy: 82.87%\n",
            "[EVAL] step:  503 | accuracy: 82.87%\n",
            "[EVAL] step:  504 | accuracy: 82.83%\n",
            "[EVAL] step:  505 | accuracy: 82.82%\n",
            "[EVAL] step:  506 | accuracy: 82.83%\n",
            "[EVAL] step:  507 | accuracy: 82.84%\n",
            "[EVAL] step:  508 | accuracy: 82.83%\n",
            "[EVAL] step:  509 | accuracy: 82.79%\n",
            "[EVAL] step:  510 | accuracy: 82.77%\n",
            "[EVAL] step:  511 | accuracy: 82.78%\n",
            "[EVAL] step:  512 | accuracy: 82.76%\n",
            "[EVAL] step:  513 | accuracy: 82.77%\n",
            "[EVAL] step:  514 | accuracy: 82.77%\n",
            "[EVAL] step:  515 | accuracy: 82.77%\n",
            "[EVAL] step:  516 | accuracy: 82.77%\n",
            "[EVAL] step:  517 | accuracy: 82.75%\n",
            "[EVAL] step:  518 | accuracy: 82.73%\n",
            "[EVAL] step:  519 | accuracy: 82.76%\n",
            "[EVAL] step:  520 | accuracy: 82.75%\n",
            "[EVAL] step:  521 | accuracy: 82.76%\n",
            "[EVAL] step:  522 | accuracy: 82.79%\n",
            "[EVAL] step:  523 | accuracy: 82.77%\n",
            "[EVAL] step:  524 | accuracy: 82.77%\n",
            "[EVAL] step:  525 | accuracy: 82.75%\n",
            "[EVAL] step:  526 | accuracy: 82.72%\n",
            "[EVAL] step:  527 | accuracy: 82.69%\n",
            "[EVAL] step:  528 | accuracy: 82.70%\n",
            "[EVAL] step:  529 | accuracy: 82.71%\n",
            "[EVAL] step:  530 | accuracy: 82.72%\n",
            "[EVAL] step:  531 | accuracy: 82.74%\n",
            "[EVAL] step:  532 | accuracy: 82.72%\n",
            "[EVAL] step:  533 | accuracy: 82.72%\n",
            "[EVAL] step:  534 | accuracy: 82.72%\n",
            "[EVAL] step:  535 | accuracy: 82.70%\n",
            "[EVAL] step:  536 | accuracy: 82.71%\n",
            "[EVAL] step:  537 | accuracy: 82.72%\n",
            "[EVAL] step:  538 | accuracy: 82.71%\n",
            "[EVAL] step:  539 | accuracy: 82.75%\n",
            "[EVAL] step:  540 | accuracy: 82.75%\n",
            "[EVAL] step:  541 | accuracy: 82.70%\n",
            "[EVAL] step:  542 | accuracy: 82.72%\n",
            "[EVAL] step:  543 | accuracy: 82.71%\n",
            "[EVAL] step:  544 | accuracy: 82.72%\n",
            "[EVAL] step:  545 | accuracy: 82.75%\n",
            "[EVAL] step:  546 | accuracy: 82.76%\n",
            "[EVAL] step:  547 | accuracy: 82.76%\n",
            "[EVAL] step:  548 | accuracy: 82.76%\n",
            "[EVAL] step:  549 | accuracy: 82.75%\n",
            "[EVAL] step:  550 | accuracy: 82.75%\n",
            "[EVAL] step:  551 | accuracy: 82.75%\n",
            "[EVAL] step:  552 | accuracy: 82.76%\n",
            "[EVAL] step:  553 | accuracy: 82.77%\n",
            "[EVAL] step:  554 | accuracy: 82.78%\n",
            "[EVAL] step:  555 | accuracy: 82.78%\n",
            "[EVAL] step:  556 | accuracy: 82.79%\n",
            "[EVAL] step:  557 | accuracy: 82.78%\n",
            "[EVAL] step:  558 | accuracy: 82.78%\n",
            "[EVAL] step:  559 | accuracy: 82.78%\n",
            "[EVAL] step:  560 | accuracy: 82.79%\n",
            "[EVAL] step:  561 | accuracy: 82.81%\n",
            "[EVAL] step:  562 | accuracy: 82.80%\n",
            "[EVAL] step:  563 | accuracy: 82.82%\n",
            "[EVAL] step:  564 | accuracy: 82.83%\n",
            "[EVAL] step:  565 | accuracy: 82.85%\n",
            "[EVAL] step:  566 | accuracy: 82.85%\n",
            "[EVAL] step:  567 | accuracy: 82.87%\n",
            "[EVAL] step:  568 | accuracy: 82.86%\n",
            "[EVAL] step:  569 | accuracy: 82.88%\n",
            "[EVAL] step:  570 | accuracy: 82.85%\n",
            "[EVAL] step:  571 | accuracy: 82.83%\n",
            "[EVAL] step:  572 | accuracy: 82.83%\n",
            "[EVAL] step:  573 | accuracy: 82.84%\n",
            "[EVAL] step:  574 | accuracy: 82.82%\n",
            "[EVAL] step:  575 | accuracy: 82.82%\n",
            "[EVAL] step:  576 | accuracy: 82.82%\n",
            "[EVAL] step:  577 | accuracy: 82.82%\n",
            "[EVAL] step:  578 | accuracy: 82.83%\n",
            "[EVAL] step:  579 | accuracy: 82.85%\n",
            "[EVAL] step:  580 | accuracy: 82.84%\n",
            "[EVAL] step:  581 | accuracy: 82.83%\n",
            "[EVAL] step:  582 | accuracy: 82.84%\n",
            "[EVAL] step:  583 | accuracy: 82.86%\n",
            "[EVAL] step:  584 | accuracy: 82.86%\n",
            "[EVAL] step:  585 | accuracy: 82.87%\n",
            "[EVAL] step:  586 | accuracy: 82.88%\n",
            "[EVAL] step:  587 | accuracy: 82.86%\n",
            "[EVAL] step:  588 | accuracy: 82.87%\n",
            "[EVAL] step:  589 | accuracy: 82.88%\n",
            "[EVAL] step:  590 | accuracy: 82.90%\n",
            "[EVAL] step:  591 | accuracy: 82.89%\n",
            "[EVAL] step:  592 | accuracy: 82.91%\n",
            "[EVAL] step:  593 | accuracy: 82.93%\n",
            "[EVAL] step:  594 | accuracy: 82.94%\n",
            "[EVAL] step:  595 | accuracy: 82.94%\n",
            "[EVAL] step:  596 | accuracy: 82.94%\n",
            "[EVAL] step:  597 | accuracy: 82.92%\n",
            "[EVAL] step:  598 | accuracy: 82.92%\n",
            "[EVAL] step:  599 | accuracy: 82.93%\n",
            "[EVAL] step:  600 | accuracy: 82.93%\n",
            "[EVAL] step:  601 | accuracy: 82.88%\n",
            "[EVAL] step:  602 | accuracy: 82.88%\n",
            "[EVAL] step:  603 | accuracy: 82.88%\n",
            "[EVAL] step:  604 | accuracy: 82.90%\n",
            "[EVAL] step:  605 | accuracy: 82.88%\n",
            "[EVAL] step:  606 | accuracy: 82.86%\n",
            "[EVAL] step:  607 | accuracy: 82.83%\n",
            "[EVAL] step:  608 | accuracy: 82.81%\n",
            "[EVAL] step:  609 | accuracy: 82.82%\n",
            "[EVAL] step:  610 | accuracy: 82.84%\n",
            "[EVAL] step:  611 | accuracy: 82.86%\n",
            "[EVAL] step:  612 | accuracy: 82.84%\n",
            "[EVAL] step:  613 | accuracy: 82.84%\n",
            "[EVAL] step:  614 | accuracy: 82.83%\n",
            "[EVAL] step:  615 | accuracy: 82.84%\n",
            "[EVAL] step:  616 | accuracy: 82.85%\n",
            "[EVAL] step:  617 | accuracy: 82.82%\n",
            "[EVAL] step:  618 | accuracy: 82.83%\n",
            "[EVAL] step:  619 | accuracy: 82.81%\n",
            "[EVAL] step:  620 | accuracy: 82.81%\n",
            "[EVAL] step:  621 | accuracy: 82.81%\n",
            "[EVAL] step:  622 | accuracy: 82.82%\n",
            "[EVAL] step:  623 | accuracy: 82.83%\n",
            "[EVAL] step:  624 | accuracy: 82.84%\n",
            "[EVAL] step:  625 | accuracy: 82.84%\n",
            "[EVAL] step:  626 | accuracy: 82.85%\n",
            "[EVAL] step:  627 | accuracy: 82.86%\n",
            "[EVAL] step:  628 | accuracy: 82.87%\n",
            "[EVAL] step:  629 | accuracy: 82.85%\n",
            "[EVAL] step:  630 | accuracy: 82.86%\n",
            "[EVAL] step:  631 | accuracy: 82.84%\n",
            "[EVAL] step:  632 | accuracy: 82.82%\n",
            "[EVAL] step:  633 | accuracy: 82.83%\n",
            "[EVAL] step:  634 | accuracy: 82.82%\n",
            "[EVAL] step:  635 | accuracy: 82.80%\n",
            "[EVAL] step:  636 | accuracy: 82.81%\n",
            "[EVAL] step:  637 | accuracy: 82.79%\n",
            "[EVAL] step:  638 | accuracy: 82.77%\n",
            "[EVAL] step:  639 | accuracy: 82.77%\n",
            "[EVAL] step:  640 | accuracy: 82.78%\n",
            "[EVAL] step:  641 | accuracy: 82.76%\n",
            "[EVAL] step:  642 | accuracy: 82.77%\n",
            "[EVAL] step:  643 | accuracy: 82.79%\n",
            "[EVAL] step:  644 | accuracy: 82.80%\n",
            "[EVAL] step:  645 | accuracy: 82.80%\n",
            "[EVAL] step:  646 | accuracy: 82.81%\n",
            "[EVAL] step:  647 | accuracy: 82.80%\n",
            "[EVAL] step:  648 | accuracy: 82.79%\n",
            "[EVAL] step:  649 | accuracy: 82.79%\n",
            "[EVAL] step:  650 | accuracy: 82.81%\n",
            "[EVAL] step:  651 | accuracy: 82.80%\n",
            "[EVAL] step:  652 | accuracy: 82.80%\n",
            "[EVAL] step:  653 | accuracy: 82.79%\n",
            "[EVAL] step:  654 | accuracy: 82.78%\n",
            "[EVAL] step:  655 | accuracy: 82.76%\n",
            "[EVAL] step:  656 | accuracy: 82.74%\n",
            "[EVAL] step:  657 | accuracy: 82.72%\n",
            "[EVAL] step:  658 | accuracy: 82.68%\n",
            "[EVAL] step:  659 | accuracy: 82.68%\n",
            "[EVAL] step:  660 | accuracy: 82.69%\n",
            "[EVAL] step:  661 | accuracy: 82.72%\n",
            "[EVAL] step:  662 | accuracy: 82.71%\n",
            "[EVAL] step:  663 | accuracy: 82.71%\n",
            "[EVAL] step:  664 | accuracy: 82.71%\n",
            "[EVAL] step:  665 | accuracy: 82.70%\n",
            "[EVAL] step:  666 | accuracy: 82.70%\n",
            "[EVAL] step:  667 | accuracy: 82.70%\n",
            "[EVAL] step:  668 | accuracy: 82.71%\n",
            "[EVAL] step:  669 | accuracy: 82.72%\n",
            "[EVAL] step:  670 | accuracy: 82.70%\n",
            "[EVAL] step:  671 | accuracy: 82.70%\n",
            "[EVAL] step:  672 | accuracy: 82.71%\n",
            "[EVAL] step:  673 | accuracy: 82.73%\n",
            "[EVAL] step:  674 | accuracy: 82.73%\n",
            "[EVAL] step:  675 | accuracy: 82.73%\n",
            "[EVAL] step:  676 | accuracy: 82.74%\n",
            "[EVAL] step:  677 | accuracy: 82.75%\n",
            "[EVAL] step:  678 | accuracy: 82.77%\n",
            "[EVAL] step:  679 | accuracy: 82.74%\n",
            "[EVAL] step:  680 | accuracy: 82.74%\n",
            "[EVAL] step:  681 | accuracy: 82.75%\n",
            "[EVAL] step:  682 | accuracy: 82.75%\n",
            "[EVAL] step:  683 | accuracy: 82.75%\n",
            "[EVAL] step:  684 | accuracy: 82.75%\n",
            "[EVAL] step:  685 | accuracy: 82.75%\n",
            "[EVAL] step:  686 | accuracy: 82.76%\n",
            "[EVAL] step:  687 | accuracy: 82.76%\n",
            "[EVAL] step:  688 | accuracy: 82.74%\n",
            "[EVAL] step:  689 | accuracy: 82.75%\n",
            "[EVAL] step:  690 | accuracy: 82.72%\n",
            "[EVAL] step:  691 | accuracy: 82.69%\n",
            "[EVAL] step:  692 | accuracy: 82.70%\n",
            "[EVAL] step:  693 | accuracy: 82.69%\n",
            "[EVAL] step:  694 | accuracy: 82.69%\n",
            "[EVAL] step:  695 | accuracy: 82.68%\n",
            "[EVAL] step:  696 | accuracy: 82.69%\n",
            "[EVAL] step:  697 | accuracy: 82.69%\n",
            "[EVAL] step:  698 | accuracy: 82.69%\n",
            "[EVAL] step:  699 | accuracy: 82.69%\n",
            "[EVAL] step:  700 | accuracy: 82.70%\n",
            "[EVAL] step:  701 | accuracy: 82.69%\n",
            "[EVAL] step:  702 | accuracy: 82.69%\n",
            "[EVAL] step:  703 | accuracy: 82.67%\n",
            "[EVAL] step:  704 | accuracy: 82.69%\n",
            "[EVAL] step:  705 | accuracy: 82.70%\n",
            "[EVAL] step:  706 | accuracy: 82.68%\n",
            "[EVAL] step:  707 | accuracy: 82.69%\n",
            "[EVAL] step:  708 | accuracy: 82.69%\n",
            "[EVAL] step:  709 | accuracy: 82.70%\n",
            "[EVAL] step:  710 | accuracy: 82.69%\n",
            "[EVAL] step:  711 | accuracy: 82.67%\n",
            "[EVAL] step:  712 | accuracy: 82.69%\n",
            "[EVAL] step:  713 | accuracy: 82.71%\n",
            "[EVAL] step:  714 | accuracy: 82.71%\n",
            "[EVAL] step:  715 | accuracy: 82.73%\n",
            "[EVAL] step:  716 | accuracy: 82.74%\n",
            "[EVAL] step:  717 | accuracy: 82.73%\n",
            "[EVAL] step:  718 | accuracy: 82.74%\n",
            "[EVAL] step:  719 | accuracy: 82.73%\n",
            "[EVAL] step:  720 | accuracy: 82.72%\n",
            "[EVAL] step:  721 | accuracy: 82.70%\n",
            "[EVAL] step:  722 | accuracy: 82.71%\n",
            "[EVAL] step:  723 | accuracy: 82.71%\n",
            "[EVAL] step:  724 | accuracy: 82.69%\n",
            "[EVAL] step:  725 | accuracy: 82.71%\n",
            "[EVAL] step:  726 | accuracy: 82.71%\n",
            "[EVAL] step:  727 | accuracy: 82.72%\n",
            "[EVAL] step:  728 | accuracy: 82.72%\n",
            "[EVAL] step:  729 | accuracy: 82.70%\n",
            "[EVAL] step:  730 | accuracy: 82.71%\n",
            "[EVAL] step:  731 | accuracy: 82.69%\n",
            "[EVAL] step:  732 | accuracy: 82.67%\n",
            "[EVAL] step:  733 | accuracy: 82.66%\n",
            "[EVAL] step:  734 | accuracy: 82.68%\n",
            "[EVAL] step:  735 | accuracy: 82.69%\n",
            "[EVAL] step:  736 | accuracy: 82.69%\n",
            "[EVAL] step:  737 | accuracy: 82.69%\n",
            "[EVAL] step:  738 | accuracy: 82.69%\n",
            "[EVAL] step:  739 | accuracy: 82.71%\n",
            "[EVAL] step:  740 | accuracy: 82.72%\n",
            "[EVAL] step:  741 | accuracy: 82.72%\n",
            "[EVAL] step:  742 | accuracy: 82.72%\n",
            "[EVAL] step:  743 | accuracy: 82.71%\n",
            "[EVAL] step:  744 | accuracy: 82.72%\n",
            "[EVAL] step:  745 | accuracy: 82.70%\n",
            "[EVAL] step:  746 | accuracy: 82.70%\n",
            "[EVAL] step:  747 | accuracy: 82.69%\n",
            "[EVAL] step:  748 | accuracy: 82.69%\n",
            "[EVAL] step:  749 | accuracy: 82.69%\n",
            "[EVAL] step:  750 | accuracy: 82.69%\n",
            "[EVAL] step:  751 | accuracy: 82.68%\n",
            "[EVAL] step:  752 | accuracy: 82.67%\n",
            "[EVAL] step:  753 | accuracy: 82.68%\n",
            "[EVAL] step:  754 | accuracy: 82.68%\n",
            "[EVAL] step:  755 | accuracy: 82.68%\n",
            "[EVAL] step:  756 | accuracy: 82.67%\n",
            "[EVAL] step:  757 | accuracy: 82.67%\n",
            "[EVAL] step:  758 | accuracy: 82.68%\n",
            "[EVAL] step:  759 | accuracy: 82.69%\n",
            "[EVAL] step:  760 | accuracy: 82.69%\n",
            "[EVAL] step:  761 | accuracy: 82.70%\n",
            "[EVAL] step:  762 | accuracy: 82.70%\n",
            "[EVAL] step:  763 | accuracy: 82.68%\n",
            "[EVAL] step:  764 | accuracy: 82.68%\n",
            "[EVAL] step:  765 | accuracy: 82.67%\n",
            "[EVAL] step:  766 | accuracy: 82.66%\n",
            "[EVAL] step:  767 | accuracy: 82.65%\n",
            "[EVAL] step:  768 | accuracy: 82.64%\n",
            "[EVAL] step:  769 | accuracy: 82.63%\n",
            "[EVAL] step:  770 | accuracy: 82.64%\n",
            "[EVAL] step:  771 | accuracy: 82.64%\n",
            "[EVAL] step:  772 | accuracy: 82.64%\n",
            "[EVAL] step:  773 | accuracy: 82.65%\n",
            "[EVAL] step:  774 | accuracy: 82.65%\n",
            "[EVAL] step:  775 | accuracy: 82.66%\n",
            "[EVAL] step:  776 | accuracy: 82.67%\n",
            "[EVAL] step:  777 | accuracy: 82.66%\n",
            "[EVAL] step:  778 | accuracy: 82.67%\n",
            "[EVAL] step:  779 | accuracy: 82.68%\n",
            "[EVAL] step:  780 | accuracy: 82.68%\n",
            "[EVAL] step:  781 | accuracy: 82.67%\n",
            "[EVAL] step:  782 | accuracy: 82.66%\n",
            "[EVAL] step:  783 | accuracy: 82.66%\n",
            "[EVAL] step:  784 | accuracy: 82.66%\n",
            "[EVAL] step:  785 | accuracy: 82.66%\n",
            "[EVAL] step:  786 | accuracy: 82.66%\n",
            "[EVAL] step:  787 | accuracy: 82.63%\n",
            "[EVAL] step:  788 | accuracy: 82.63%\n",
            "[EVAL] step:  789 | accuracy: 82.64%\n",
            "[EVAL] step:  790 | accuracy: 82.65%\n",
            "[EVAL] step:  791 | accuracy: 82.65%\n",
            "[EVAL] step:  792 | accuracy: 82.66%\n",
            "[EVAL] step:  793 | accuracy: 82.66%\n",
            "[EVAL] step:  794 | accuracy: 82.66%\n",
            "[EVAL] step:  795 | accuracy: 82.66%\n",
            "[EVAL] step:  796 | accuracy: 82.64%\n",
            "[EVAL] step:  797 | accuracy: 82.64%\n",
            "[EVAL] step:  798 | accuracy: 82.64%\n",
            "[EVAL] step:  799 | accuracy: 82.64%\n",
            "[EVAL] step:  800 | accuracy: 82.65%\n",
            "[EVAL] step:  801 | accuracy: 82.66%\n",
            "[EVAL] step:  802 | accuracy: 82.67%\n",
            "[EVAL] step:  803 | accuracy: 82.67%\n",
            "[EVAL] step:  804 | accuracy: 82.67%\n",
            "[EVAL] step:  805 | accuracy: 82.67%\n",
            "[EVAL] step:  806 | accuracy: 82.67%\n",
            "[EVAL] step:  807 | accuracy: 82.67%\n",
            "[EVAL] step:  808 | accuracy: 82.66%\n",
            "[EVAL] step:  809 | accuracy: 82.66%\n",
            "[EVAL] step:  810 | accuracy: 82.65%\n",
            "[EVAL] step:  811 | accuracy: 82.65%\n",
            "[EVAL] step:  812 | accuracy: 82.65%\n",
            "[EVAL] step:  813 | accuracy: 82.64%\n",
            "[EVAL] step:  814 | accuracy: 82.64%\n",
            "[EVAL] step:  815 | accuracy: 82.65%\n",
            "[EVAL] step:  816 | accuracy: 82.64%\n",
            "[EVAL] step:  817 | accuracy: 82.63%\n",
            "[EVAL] step:  818 | accuracy: 82.63%\n",
            "[EVAL] step:  819 | accuracy: 82.64%\n",
            "[EVAL] step:  820 | accuracy: 82.63%\n",
            "[EVAL] step:  821 | accuracy: 82.64%\n",
            "[EVAL] step:  822 | accuracy: 82.65%\n",
            "[EVAL] step:  823 | accuracy: 82.64%\n",
            "[EVAL] step:  824 | accuracy: 82.65%\n",
            "[EVAL] step:  825 | accuracy: 82.65%\n",
            "[EVAL] step:  826 | accuracy: 82.64%\n",
            "[EVAL] step:  827 | accuracy: 82.65%\n",
            "[EVAL] step:  828 | accuracy: 82.64%\n",
            "[EVAL] step:  829 | accuracy: 82.65%\n",
            "[EVAL] step:  830 | accuracy: 82.66%\n",
            "[EVAL] step:  831 | accuracy: 82.65%\n",
            "[EVAL] step:  832 | accuracy: 82.63%\n",
            "[EVAL] step:  833 | accuracy: 82.62%\n",
            "[EVAL] step:  834 | accuracy: 82.61%\n",
            "[EVAL] step:  835 | accuracy: 82.62%\n",
            "[EVAL] step:  836 | accuracy: 82.60%\n",
            "[EVAL] step:  837 | accuracy: 82.59%\n",
            "[EVAL] step:  838 | accuracy: 82.58%\n",
            "[EVAL] step:  839 | accuracy: 82.59%\n",
            "[EVAL] step:  840 | accuracy: 82.59%\n",
            "[EVAL] step:  841 | accuracy: 82.59%\n",
            "[EVAL] step:  842 | accuracy: 82.60%\n",
            "[EVAL] step:  843 | accuracy: 82.60%\n",
            "[EVAL] step:  844 | accuracy: 82.61%\n",
            "[EVAL] step:  845 | accuracy: 82.61%\n",
            "[EVAL] step:  846 | accuracy: 82.59%\n",
            "[EVAL] step:  847 | accuracy: 82.59%\n",
            "[EVAL] step:  848 | accuracy: 82.60%\n",
            "[EVAL] step:  849 | accuracy: 82.60%\n",
            "[EVAL] step:  850 | accuracy: 82.62%\n",
            "[EVAL] step:  851 | accuracy: 82.61%\n",
            "[EVAL] step:  852 | accuracy: 82.62%\n",
            "[EVAL] step:  853 | accuracy: 82.61%\n",
            "[EVAL] step:  854 | accuracy: 82.61%\n",
            "[EVAL] step:  855 | accuracy: 82.61%\n",
            "[EVAL] step:  856 | accuracy: 82.62%\n",
            "[EVAL] step:  857 | accuracy: 82.61%\n",
            "[EVAL] step:  858 | accuracy: 82.60%\n",
            "[EVAL] step:  859 | accuracy: 82.60%\n",
            "[EVAL] step:  860 | accuracy: 82.60%\n",
            "[EVAL] step:  861 | accuracy: 82.59%\n",
            "[EVAL] step:  862 | accuracy: 82.58%\n",
            "[EVAL] step:  863 | accuracy: 82.57%\n",
            "[EVAL] step:  864 | accuracy: 82.56%\n",
            "[EVAL] step:  865 | accuracy: 82.57%\n",
            "[EVAL] step:  866 | accuracy: 82.58%\n",
            "[EVAL] step:  867 | accuracy: 82.59%\n",
            "[EVAL] step:  868 | accuracy: 82.58%\n",
            "[EVAL] step:  869 | accuracy: 82.57%\n",
            "[EVAL] step:  870 | accuracy: 82.57%\n",
            "[EVAL] step:  871 | accuracy: 82.57%\n",
            "[EVAL] step:  872 | accuracy: 82.56%\n",
            "[EVAL] step:  873 | accuracy: 82.57%\n",
            "[EVAL] step:  874 | accuracy: 82.57%\n",
            "[EVAL] step:  875 | accuracy: 82.57%\n",
            "[EVAL] step:  876 | accuracy: 82.57%\n",
            "[EVAL] step:  877 | accuracy: 82.57%\n",
            "[EVAL] step:  878 | accuracy: 82.57%\n",
            "[EVAL] step:  879 | accuracy: 82.57%\n",
            "[EVAL] step:  880 | accuracy: 82.57%\n",
            "[EVAL] step:  881 | accuracy: 82.57%\n",
            "[EVAL] step:  882 | accuracy: 82.55%\n",
            "[EVAL] step:  883 | accuracy: 82.55%\n",
            "[EVAL] step:  884 | accuracy: 82.56%\n",
            "[EVAL] step:  885 | accuracy: 82.55%\n",
            "[EVAL] step:  886 | accuracy: 82.55%\n",
            "[EVAL] step:  887 | accuracy: 82.55%\n",
            "[EVAL] step:  888 | accuracy: 82.55%\n",
            "[EVAL] step:  889 | accuracy: 82.56%\n",
            "[EVAL] step:  890 | accuracy: 82.56%\n",
            "[EVAL] step:  891 | accuracy: 82.56%\n",
            "[EVAL] step:  892 | accuracy: 82.55%\n",
            "[EVAL] step:  893 | accuracy: 82.56%\n",
            "[EVAL] step:  894 | accuracy: 82.57%\n",
            "[EVAL] step:  895 | accuracy: 82.56%\n",
            "[EVAL] step:  896 | accuracy: 82.55%\n",
            "[EVAL] step:  897 | accuracy: 82.54%\n",
            "[EVAL] step:  898 | accuracy: 82.56%\n",
            "[EVAL] step:  899 | accuracy: 82.55%\n",
            "[EVAL] step:  900 | accuracy: 82.56%\n",
            "[EVAL] step:  901 | accuracy: 82.55%\n",
            "[EVAL] step:  902 | accuracy: 82.54%\n",
            "[EVAL] step:  903 | accuracy: 82.54%\n",
            "[EVAL] step:  904 | accuracy: 82.54%\n",
            "[EVAL] step:  905 | accuracy: 82.54%\n",
            "[EVAL] step:  906 | accuracy: 82.52%\n",
            "[EVAL] step:  907 | accuracy: 82.54%\n",
            "[EVAL] step:  908 | accuracy: 82.53%\n",
            "[EVAL] step:  909 | accuracy: 82.54%\n",
            "[EVAL] step:  910 | accuracy: 82.53%\n",
            "[EVAL] step:  911 | accuracy: 82.52%\n",
            "[EVAL] step:  912 | accuracy: 82.52%\n",
            "[EVAL] step:  913 | accuracy: 82.50%\n",
            "[EVAL] step:  914 | accuracy: 82.51%\n",
            "[EVAL] step:  915 | accuracy: 82.50%\n",
            "[EVAL] step:  916 | accuracy: 82.51%\n",
            "[EVAL] step:  917 | accuracy: 82.50%\n",
            "[EVAL] step:  918 | accuracy: 82.50%\n",
            "[EVAL] step:  919 | accuracy: 82.51%\n",
            "[EVAL] step:  920 | accuracy: 82.52%\n",
            "[EVAL] step:  921 | accuracy: 82.51%\n",
            "[EVAL] step:  922 | accuracy: 82.50%\n",
            "[EVAL] step:  923 | accuracy: 82.50%\n",
            "[EVAL] step:  924 | accuracy: 82.50%\n",
            "[EVAL] step:  925 | accuracy: 82.50%\n",
            "[EVAL] step:  926 | accuracy: 82.51%\n",
            "[EVAL] step:  927 | accuracy: 82.52%\n",
            "[EVAL] step:  928 | accuracy: 82.53%\n",
            "[EVAL] step:  929 | accuracy: 82.53%\n",
            "[EVAL] step:  930 | accuracy: 82.53%\n",
            "[EVAL] step:  931 | accuracy: 82.52%\n",
            "[EVAL] step:  932 | accuracy: 82.52%\n",
            "[EVAL] step:  933 | accuracy: 82.53%\n",
            "[EVAL] step:  934 | accuracy: 82.53%\n",
            "[EVAL] step:  935 | accuracy: 82.52%\n",
            "[EVAL] step:  936 | accuracy: 82.51%\n",
            "[EVAL] step:  937 | accuracy: 82.51%\n",
            "[EVAL] step:  938 | accuracy: 82.51%\n",
            "[EVAL] step:  939 | accuracy: 82.51%\n",
            "[EVAL] step:  940 | accuracy: 82.50%\n",
            "[EVAL] step:  941 | accuracy: 82.50%\n",
            "[EVAL] step:  942 | accuracy: 82.51%\n",
            "[EVAL] step:  943 | accuracy: 82.51%\n",
            "[EVAL] step:  944 | accuracy: 82.51%\n",
            "[EVAL] step:  945 | accuracy: 82.49%\n",
            "[EVAL] step:  946 | accuracy: 82.51%\n",
            "[EVAL] step:  947 | accuracy: 82.51%\n",
            "[EVAL] step:  948 | accuracy: 82.52%\n",
            "[EVAL] step:  949 | accuracy: 82.52%\n",
            "[EVAL] step:  950 | accuracy: 82.53%\n",
            "[EVAL] step:  951 | accuracy: 82.52%\n",
            "[EVAL] step:  952 | accuracy: 82.53%\n",
            "[EVAL] step:  953 | accuracy: 82.52%\n",
            "[EVAL] step:  954 | accuracy: 82.53%\n",
            "[EVAL] step:  955 | accuracy: 82.52%\n",
            "[EVAL] step:  956 | accuracy: 82.52%\n",
            "[EVAL] step:  957 | accuracy: 82.52%\n",
            "[EVAL] step:  958 | accuracy: 82.53%\n",
            "[EVAL] step:  959 | accuracy: 82.53%\n",
            "[EVAL] step:  960 | accuracy: 82.53%\n",
            "[EVAL] step:  961 | accuracy: 82.53%\n",
            "[EVAL] step:  962 | accuracy: 82.53%\n",
            "[EVAL] step:  963 | accuracy: 82.53%\n",
            "[EVAL] step:  964 | accuracy: 82.53%\n",
            "[EVAL] step:  965 | accuracy: 82.53%\n",
            "[EVAL] step:  966 | accuracy: 82.54%\n",
            "[EVAL] step:  967 | accuracy: 82.54%\n",
            "[EVAL] step:  968 | accuracy: 82.53%\n",
            "[EVAL] step:  969 | accuracy: 82.53%\n",
            "[EVAL] step:  970 | accuracy: 82.53%\n",
            "[EVAL] step:  971 | accuracy: 82.53%\n",
            "[EVAL] step:  972 | accuracy: 82.53%\n",
            "[EVAL] step:  973 | accuracy: 82.54%\n",
            "[EVAL] step:  974 | accuracy: 82.54%\n",
            "[EVAL] step:  975 | accuracy: 82.55%\n",
            "[EVAL] step:  976 | accuracy: 82.56%\n",
            "[EVAL] step:  977 | accuracy: 82.57%\n",
            "[EVAL] step:  978 | accuracy: 82.57%\n",
            "[EVAL] step:  979 | accuracy: 82.56%\n",
            "[EVAL] step:  980 | accuracy: 82.57%\n",
            "[EVAL] step:  981 | accuracy: 82.57%\n",
            "[EVAL] step:  982 | accuracy: 82.58%\n",
            "[EVAL] step:  983 | accuracy: 82.59%\n",
            "[EVAL] step:  984 | accuracy: 82.60%\n",
            "[EVAL] step:  985 | accuracy: 82.59%\n",
            "[EVAL] step:  986 | accuracy: 82.60%\n",
            "[EVAL] step:  987 | accuracy: 82.58%\n",
            "[EVAL] step:  988 | accuracy: 82.57%\n",
            "[EVAL] step:  989 | accuracy: 82.58%\n",
            "[EVAL] step:  990 | accuracy: 82.58%\n",
            "[EVAL] step:  991 | accuracy: 82.56%\n",
            "[EVAL] step:  992 | accuracy: 82.57%\n",
            "[EVAL] step:  993 | accuracy: 82.58%\n",
            "[EVAL] step:  994 | accuracy: 82.58%\n",
            "[EVAL] step:  995 | accuracy: 82.58%\n",
            "[EVAL] step:  996 | accuracy: 82.58%\n",
            "[EVAL] step:  997 | accuracy: 82.57%\n",
            "[EVAL] step:  998 | accuracy: 82.57%\n",
            "[EVAL] step:  999 | accuracy: 82.55%\n",
            "[EVAL] step: 1000 | accuracy: 82.56%\n",
            "\n",
            "Best checkpoint\n",
            "step: 1001 | loss: 0.087811, accuracy: 95.00%\n",
            "step: 1002 | loss: 0.123342, accuracy: 95.00%\n",
            "step: 1003 | loss: 0.103794, accuracy: 95.00%\n",
            "step: 1004 | loss: 0.148688, accuracy: 93.75%\n",
            "step: 1005 | loss: 0.132079, accuracy: 95.00%\n",
            "step: 1006 | loss: 0.163520, accuracy: 93.33%\n",
            "step: 1007 | loss: 0.192929, accuracy: 92.86%\n",
            "step: 1008 | loss: 0.182865, accuracy: 93.12%\n",
            "step: 1009 | loss: 0.180439, accuracy: 92.78%\n",
            "step: 1010 | loss: 0.187693, accuracy: 93.00%\n",
            "step: 1011 | loss: 0.177779, accuracy: 93.64%\n",
            "step: 1012 | loss: 0.166406, accuracy: 94.17%\n",
            "step: 1013 | loss: 0.154522, accuracy: 94.62%\n",
            "step: 1014 | loss: 0.152125, accuracy: 94.64%\n",
            "step: 1015 | loss: 0.173546, accuracy: 94.00%\n",
            "step: 1016 | loss: 0.165085, accuracy: 94.37%\n",
            "step: 1017 | loss: 0.160192, accuracy: 94.41%\n",
            "step: 1018 | loss: 0.175408, accuracy: 94.17%\n",
            "step: 1019 | loss: 0.176918, accuracy: 94.21%\n",
            "step: 1020 | loss: 0.172225, accuracy: 94.25%\n",
            "step: 1021 | loss: 0.171005, accuracy: 94.29%\n",
            "step: 1022 | loss: 0.186262, accuracy: 93.64%\n",
            "step: 1023 | loss: 0.179968, accuracy: 93.91%\n",
            "step: 1024 | loss: 0.178139, accuracy: 93.96%\n",
            "step: 1025 | loss: 0.178233, accuracy: 94.00%\n",
            "step: 1026 | loss: 0.182489, accuracy: 94.04%\n",
            "step: 1027 | loss: 0.178468, accuracy: 94.07%\n",
            "step: 1028 | loss: 0.175149, accuracy: 94.11%\n",
            "step: 1029 | loss: 0.169253, accuracy: 94.31%\n",
            "step: 1030 | loss: 0.179257, accuracy: 94.33%\n",
            "step: 1031 | loss: 0.180669, accuracy: 94.35%\n",
            "step: 1032 | loss: 0.189746, accuracy: 94.06%\n",
            "step: 1033 | loss: 0.184565, accuracy: 94.24%\n",
            "step: 1034 | loss: 0.186912, accuracy: 94.12%\n",
            "step: 1035 | loss: 0.187217, accuracy: 94.14%\n",
            "step: 1036 | loss: 0.189718, accuracy: 93.89%\n",
            "step: 1037 | loss: 0.187712, accuracy: 93.92%\n",
            "step: 1038 | loss: 0.193023, accuracy: 93.82%\n",
            "step: 1039 | loss: 0.191569, accuracy: 93.85%\n",
            "step: 1040 | loss: 0.187613, accuracy: 94.00%\n",
            "step: 1041 | loss: 0.187490, accuracy: 94.02%\n",
            "step: 1042 | loss: 0.186253, accuracy: 93.93%\n",
            "step: 1043 | loss: 0.184958, accuracy: 93.95%\n",
            "step: 1044 | loss: 0.181664, accuracy: 94.09%\n",
            "step: 1045 | loss: 0.181685, accuracy: 94.00%\n",
            "step: 1046 | loss: 0.179619, accuracy: 94.02%\n",
            "step: 1047 | loss: 0.181066, accuracy: 93.94%\n",
            "step: 1048 | loss: 0.179427, accuracy: 94.06%\n",
            "step: 1049 | loss: 0.181503, accuracy: 93.88%\n",
            "step: 1050 | loss: 0.180922, accuracy: 93.90%\n",
            "step: 1051 | loss: 0.179062, accuracy: 93.92%\n",
            "step: 1052 | loss: 0.178199, accuracy: 93.94%\n",
            "step: 1053 | loss: 0.176308, accuracy: 94.06%\n",
            "step: 1054 | loss: 0.178360, accuracy: 93.98%\n",
            "step: 1055 | loss: 0.183595, accuracy: 93.73%\n",
            "step: 1056 | loss: 0.181237, accuracy: 93.84%\n",
            "step: 1057 | loss: 0.180555, accuracy: 93.95%\n",
            "step: 1058 | loss: 0.179111, accuracy: 94.05%\n",
            "step: 1059 | loss: 0.178042, accuracy: 94.07%\n",
            "step: 1060 | loss: 0.176885, accuracy: 94.08%\n",
            "step: 1061 | loss: 0.174409, accuracy: 94.18%\n",
            "step: 1062 | loss: 0.173665, accuracy: 94.19%\n",
            "step: 1063 | loss: 0.179877, accuracy: 93.97%\n",
            "step: 1064 | loss: 0.181114, accuracy: 93.98%\n",
            "step: 1065 | loss: 0.184669, accuracy: 93.77%\n",
            "step: 1066 | loss: 0.187030, accuracy: 93.64%\n",
            "step: 1067 | loss: 0.184617, accuracy: 93.73%\n",
            "step: 1068 | loss: 0.188891, accuracy: 93.60%\n",
            "step: 1069 | loss: 0.190424, accuracy: 93.55%\n",
            "step: 1070 | loss: 0.189367, accuracy: 93.64%\n",
            "step: 1071 | loss: 0.187320, accuracy: 93.73%\n",
            "step: 1072 | loss: 0.189088, accuracy: 93.61%\n",
            "step: 1073 | loss: 0.186867, accuracy: 93.70%\n",
            "step: 1074 | loss: 0.192759, accuracy: 93.45%\n",
            "step: 1075 | loss: 0.190444, accuracy: 93.53%\n",
            "step: 1076 | loss: 0.191140, accuracy: 93.42%\n",
            "step: 1077 | loss: 0.192196, accuracy: 93.31%\n",
            "step: 1078 | loss: 0.191219, accuracy: 93.40%\n",
            "step: 1079 | loss: 0.191821, accuracy: 93.35%\n",
            "step: 1080 | loss: 0.191342, accuracy: 93.37%\n",
            "step: 1081 | loss: 0.191838, accuracy: 93.27%\n",
            "step: 1082 | loss: 0.192566, accuracy: 93.17%\n",
            "step: 1083 | loss: 0.193857, accuracy: 93.13%\n",
            "step: 1084 | loss: 0.193398, accuracy: 93.15%\n",
            "step: 1085 | loss: 0.194920, accuracy: 93.12%\n",
            "step: 1086 | loss: 0.195215, accuracy: 93.14%\n",
            "step: 1087 | loss: 0.194370, accuracy: 93.16%\n",
            "step: 1088 | loss: 0.193757, accuracy: 93.18%\n",
            "step: 1089 | loss: 0.193270, accuracy: 93.20%\n",
            "step: 1090 | loss: 0.191844, accuracy: 93.22%\n",
            "step: 1091 | loss: 0.190006, accuracy: 93.30%\n",
            "step: 1092 | loss: 0.189937, accuracy: 93.21%\n",
            "step: 1093 | loss: 0.192991, accuracy: 93.12%\n",
            "step: 1094 | loss: 0.191717, accuracy: 93.14%\n",
            "step: 1095 | loss: 0.189837, accuracy: 93.21%\n",
            "step: 1096 | loss: 0.189133, accuracy: 93.23%\n",
            "step: 1097 | loss: 0.190914, accuracy: 93.20%\n",
            "step: 1098 | loss: 0.192418, accuracy: 93.21%\n",
            "step: 1099 | loss: 0.194096, accuracy: 93.13%\n",
            "step: 1100 | loss: 0.193343, accuracy: 93.15%\n",
            "step: 1101 | loss: 0.194278, accuracy: 93.07%\n",
            "step: 1102 | loss: 0.194988, accuracy: 92.99%\n",
            "step: 1103 | loss: 0.197110, accuracy: 92.82%\n",
            "step: 1104 | loss: 0.198445, accuracy: 92.74%\n",
            "step: 1105 | loss: 0.197358, accuracy: 92.81%\n",
            "step: 1106 | loss: 0.196882, accuracy: 92.78%\n",
            "step: 1107 | loss: 0.196185, accuracy: 92.80%\n",
            "step: 1108 | loss: 0.195636, accuracy: 92.82%\n",
            "step: 1109 | loss: 0.195286, accuracy: 92.84%\n",
            "step: 1110 | loss: 0.194667, accuracy: 92.91%\n",
            "step: 1111 | loss: 0.197772, accuracy: 92.84%\n",
            "step: 1112 | loss: 0.197705, accuracy: 92.81%\n",
            "step: 1113 | loss: 0.197812, accuracy: 92.79%\n",
            "step: 1114 | loss: 0.199435, accuracy: 92.72%\n",
            "step: 1115 | loss: 0.201049, accuracy: 92.57%\n",
            "step: 1116 | loss: 0.200254, accuracy: 92.59%\n",
            "step: 1117 | loss: 0.199004, accuracy: 92.65%\n",
            "step: 1118 | loss: 0.200963, accuracy: 92.50%\n",
            "step: 1119 | loss: 0.203765, accuracy: 92.48%\n",
            "step: 1120 | loss: 0.203968, accuracy: 92.42%\n",
            "step: 1121 | loss: 0.203519, accuracy: 92.40%\n",
            "step: 1122 | loss: 0.202989, accuracy: 92.42%\n",
            "step: 1123 | loss: 0.204727, accuracy: 92.40%\n",
            "step: 1124 | loss: 0.204364, accuracy: 92.38%\n",
            "step: 1125 | loss: 0.205386, accuracy: 92.24%\n",
            "step: 1126 | loss: 0.205044, accuracy: 92.26%\n",
            "step: 1127 | loss: 0.204986, accuracy: 92.28%\n",
            "step: 1128 | loss: 0.204007, accuracy: 92.30%\n",
            "step: 1129 | loss: 0.205505, accuracy: 92.29%\n",
            "step: 1130 | loss: 0.204349, accuracy: 92.35%\n",
            "step: 1131 | loss: 0.205005, accuracy: 92.29%\n",
            "step: 1132 | loss: 0.203957, accuracy: 92.35%\n",
            "step: 1133 | loss: 0.205182, accuracy: 92.33%\n",
            "step: 1134 | loss: 0.204677, accuracy: 92.31%\n",
            "step: 1135 | loss: 0.203209, accuracy: 92.37%\n",
            "step: 1136 | loss: 0.203647, accuracy: 92.35%\n",
            "step: 1137 | loss: 0.203042, accuracy: 92.37%\n",
            "step: 1138 | loss: 0.202561, accuracy: 92.36%\n",
            "step: 1139 | loss: 0.201703, accuracy: 92.37%\n",
            "step: 1140 | loss: 0.201095, accuracy: 92.36%\n",
            "step: 1141 | loss: 0.201883, accuracy: 92.38%\n",
            "step: 1142 | loss: 0.201087, accuracy: 92.36%\n",
            "step: 1143 | loss: 0.202703, accuracy: 92.38%\n",
            "step: 1144 | loss: 0.201395, accuracy: 92.43%\n",
            "step: 1145 | loss: 0.201463, accuracy: 92.41%\n",
            "step: 1146 | loss: 0.200884, accuracy: 92.43%\n",
            "step: 1147 | loss: 0.201465, accuracy: 92.41%\n",
            "step: 1148 | loss: 0.201065, accuracy: 92.40%\n",
            "step: 1149 | loss: 0.201345, accuracy: 92.38%\n",
            "step: 1150 | loss: 0.200788, accuracy: 92.37%\n",
            "step: 1151 | loss: 0.203582, accuracy: 92.32%\n",
            "step: 1152 | loss: 0.203084, accuracy: 92.30%\n",
            "step: 1153 | loss: 0.202579, accuracy: 92.32%\n",
            "step: 1154 | loss: 0.204406, accuracy: 92.34%\n",
            "step: 1155 | loss: 0.205784, accuracy: 92.35%\n",
            "step: 1156 | loss: 0.207198, accuracy: 92.31%\n",
            "step: 1157 | loss: 0.207314, accuracy: 92.32%\n",
            "step: 1158 | loss: 0.206549, accuracy: 92.34%\n",
            "step: 1159 | loss: 0.206724, accuracy: 92.36%\n",
            "step: 1160 | loss: 0.208018, accuracy: 92.31%\n",
            "step: 1161 | loss: 0.206959, accuracy: 92.36%\n",
            "step: 1162 | loss: 0.209061, accuracy: 92.28%\n",
            "step: 1163 | loss: 0.209600, accuracy: 92.30%\n",
            "step: 1164 | loss: 0.211199, accuracy: 92.29%\n",
            "step: 1165 | loss: 0.212664, accuracy: 92.24%\n",
            "step: 1166 | loss: 0.212092, accuracy: 92.29%\n",
            "step: 1167 | loss: 0.213046, accuracy: 92.25%\n",
            "step: 1168 | loss: 0.212163, accuracy: 92.29%\n",
            "step: 1169 | loss: 0.211992, accuracy: 92.28%\n",
            "step: 1170 | loss: 0.211342, accuracy: 92.29%\n",
            "step: 1171 | loss: 0.211324, accuracy: 92.31%\n",
            "step: 1172 | loss: 0.211819, accuracy: 92.30%\n",
            "step: 1173 | loss: 0.211004, accuracy: 92.34%\n",
            "step: 1174 | loss: 0.210371, accuracy: 92.39%\n",
            "step: 1175 | loss: 0.210826, accuracy: 92.34%\n",
            "step: 1176 | loss: 0.210232, accuracy: 92.36%\n",
            "step: 1177 | loss: 0.209752, accuracy: 92.40%\n",
            "step: 1178 | loss: 0.209812, accuracy: 92.39%\n",
            "step: 1179 | loss: 0.209022, accuracy: 92.43%\n",
            "step: 1180 | loss: 0.208403, accuracy: 92.47%\n",
            "step: 1181 | loss: 0.208405, accuracy: 92.49%\n",
            "step: 1182 | loss: 0.208842, accuracy: 92.47%\n",
            "step: 1183 | loss: 0.208454, accuracy: 92.49%\n",
            "step: 1184 | loss: 0.208422, accuracy: 92.47%\n",
            "step: 1185 | loss: 0.208085, accuracy: 92.49%\n",
            "step: 1186 | loss: 0.208312, accuracy: 92.50%\n",
            "step: 1187 | loss: 0.208066, accuracy: 92.49%\n",
            "step: 1188 | loss: 0.207286, accuracy: 92.53%\n",
            "step: 1189 | loss: 0.207131, accuracy: 92.51%\n",
            "step: 1190 | loss: 0.207119, accuracy: 92.50%\n",
            "step: 1191 | loss: 0.206286, accuracy: 92.54%\n",
            "step: 1192 | loss: 0.205356, accuracy: 92.58%\n",
            "step: 1193 | loss: 0.205359, accuracy: 92.54%\n",
            "step: 1194 | loss: 0.204610, accuracy: 92.58%\n",
            "step: 1195 | loss: 0.205259, accuracy: 92.56%\n",
            "step: 1196 | loss: 0.204478, accuracy: 92.60%\n",
            "step: 1197 | loss: 0.204413, accuracy: 92.59%\n",
            "step: 1198 | loss: 0.204511, accuracy: 92.60%\n",
            "step: 1199 | loss: 0.205411, accuracy: 92.59%\n",
            "step: 1200 | loss: 0.205147, accuracy: 92.60%\n",
            "step: 1201 | loss: 0.205266, accuracy: 92.61%\n",
            "step: 1202 | loss: 0.204960, accuracy: 92.60%\n",
            "step: 1203 | loss: 0.204116, accuracy: 92.64%\n",
            "step: 1204 | loss: 0.203715, accuracy: 92.65%\n",
            "step: 1205 | loss: 0.203028, accuracy: 92.66%\n",
            "step: 1206 | loss: 0.202542, accuracy: 92.67%\n",
            "step: 1207 | loss: 0.201963, accuracy: 92.68%\n",
            "step: 1208 | loss: 0.201940, accuracy: 92.67%\n",
            "step: 1209 | loss: 0.202447, accuracy: 92.66%\n",
            "step: 1210 | loss: 0.202601, accuracy: 92.64%\n",
            "step: 1211 | loss: 0.202368, accuracy: 92.68%\n",
            "step: 1212 | loss: 0.203462, accuracy: 92.57%\n",
            "step: 1213 | loss: 0.202527, accuracy: 92.61%\n",
            "step: 1214 | loss: 0.202996, accuracy: 92.57%\n",
            "step: 1215 | loss: 0.202386, accuracy: 92.60%\n",
            "step: 1216 | loss: 0.203886, accuracy: 92.59%\n",
            "step: 1217 | loss: 0.203162, accuracy: 92.60%\n",
            "step: 1218 | loss: 0.203141, accuracy: 92.59%\n",
            "step: 1219 | loss: 0.203425, accuracy: 92.58%\n",
            "step: 1220 | loss: 0.202904, accuracy: 92.59%\n",
            "step: 1221 | loss: 0.202632, accuracy: 92.60%\n",
            "step: 1222 | loss: 0.202528, accuracy: 92.61%\n",
            "step: 1223 | loss: 0.202643, accuracy: 92.62%\n",
            "step: 1224 | loss: 0.202550, accuracy: 92.61%\n",
            "step: 1225 | loss: 0.201882, accuracy: 92.64%\n",
            "step: 1226 | loss: 0.201891, accuracy: 92.63%\n",
            "step: 1227 | loss: 0.201390, accuracy: 92.67%\n",
            "step: 1228 | loss: 0.201258, accuracy: 92.65%\n",
            "step: 1229 | loss: 0.201305, accuracy: 92.66%\n",
            "step: 1230 | loss: 0.201574, accuracy: 92.65%\n",
            "step: 1231 | loss: 0.201043, accuracy: 92.68%\n",
            "step: 1232 | loss: 0.200343, accuracy: 92.72%\n",
            "step: 1233 | loss: 0.199718, accuracy: 92.75%\n",
            "step: 1234 | loss: 0.200878, accuracy: 92.74%\n",
            "step: 1235 | loss: 0.201139, accuracy: 92.72%\n",
            "step: 1236 | loss: 0.200871, accuracy: 92.71%\n",
            "step: 1237 | loss: 0.200517, accuracy: 92.72%\n",
            "step: 1238 | loss: 0.200012, accuracy: 92.75%\n",
            "step: 1239 | loss: 0.200305, accuracy: 92.72%\n",
            "step: 1240 | loss: 0.199655, accuracy: 92.75%\n",
            "step: 1241 | loss: 0.199594, accuracy: 92.76%\n",
            "step: 1242 | loss: 0.199224, accuracy: 92.77%\n",
            "step: 1243 | loss: 0.198849, accuracy: 92.78%\n",
            "step: 1244 | loss: 0.198983, accuracy: 92.77%\n",
            "step: 1245 | loss: 0.198325, accuracy: 92.80%\n",
            "step: 1246 | loss: 0.198576, accuracy: 92.78%\n",
            "step: 1247 | loss: 0.198600, accuracy: 92.79%\n",
            "step: 1248 | loss: 0.199555, accuracy: 92.76%\n",
            "step: 1249 | loss: 0.201414, accuracy: 92.65%\n",
            "step: 1250 | loss: 0.200823, accuracy: 92.68%\n",
            "step: 1251 | loss: 0.200176, accuracy: 92.71%\n",
            "step: 1252 | loss: 0.199414, accuracy: 92.74%\n",
            "step: 1253 | loss: 0.198760, accuracy: 92.77%\n",
            "step: 1254 | loss: 0.198946, accuracy: 92.76%\n",
            "step: 1255 | loss: 0.198515, accuracy: 92.78%\n",
            "step: 1256 | loss: 0.198745, accuracy: 92.79%\n",
            "step: 1257 | loss: 0.199076, accuracy: 92.78%\n",
            "step: 1258 | loss: 0.200196, accuracy: 92.73%\n",
            "step: 1259 | loss: 0.200110, accuracy: 92.72%\n",
            "step: 1260 | loss: 0.199572, accuracy: 92.75%\n",
            "step: 1261 | loss: 0.200188, accuracy: 92.74%\n",
            "step: 1262 | loss: 0.199779, accuracy: 92.75%\n",
            "step: 1263 | loss: 0.199712, accuracy: 92.76%\n",
            "step: 1264 | loss: 0.199896, accuracy: 92.75%\n",
            "step: 1265 | loss: 0.199727, accuracy: 92.74%\n",
            "step: 1266 | loss: 0.199721, accuracy: 92.74%\n",
            "step: 1267 | loss: 0.199351, accuracy: 92.77%\n",
            "step: 1268 | loss: 0.199255, accuracy: 92.78%\n",
            "step: 1269 | loss: 0.199112, accuracy: 92.77%\n",
            "step: 1270 | loss: 0.198757, accuracy: 92.78%\n",
            "step: 1271 | loss: 0.198272, accuracy: 92.80%\n",
            "step: 1272 | loss: 0.198123, accuracy: 92.79%\n",
            "step: 1273 | loss: 0.197539, accuracy: 92.82%\n",
            "step: 1274 | loss: 0.196887, accuracy: 92.85%\n",
            "step: 1275 | loss: 0.196486, accuracy: 92.85%\n",
            "step: 1276 | loss: 0.195855, accuracy: 92.88%\n",
            "step: 1277 | loss: 0.196483, accuracy: 92.87%\n",
            "step: 1278 | loss: 0.195789, accuracy: 92.90%\n",
            "step: 1279 | loss: 0.196165, accuracy: 92.89%\n",
            "step: 1280 | loss: 0.195950, accuracy: 92.87%\n",
            "step: 1281 | loss: 0.195979, accuracy: 92.86%\n",
            "step: 1282 | loss: 0.195703, accuracy: 92.87%\n",
            "step: 1283 | loss: 0.195454, accuracy: 92.86%\n",
            "step: 1284 | loss: 0.195181, accuracy: 92.87%\n",
            "step: 1285 | loss: 0.195087, accuracy: 92.88%\n",
            "step: 1286 | loss: 0.194484, accuracy: 92.90%\n",
            "step: 1287 | loss: 0.194239, accuracy: 92.91%\n",
            "step: 1288 | loss: 0.194323, accuracy: 92.92%\n",
            "step: 1289 | loss: 0.193943, accuracy: 92.94%\n",
            "step: 1290 | loss: 0.194137, accuracy: 92.91%\n",
            "step: 1291 | loss: 0.194219, accuracy: 92.90%\n",
            "step: 1292 | loss: 0.195419, accuracy: 92.88%\n",
            "step: 1293 | loss: 0.195700, accuracy: 92.85%\n",
            "step: 1294 | loss: 0.195356, accuracy: 92.86%\n",
            "step: 1295 | loss: 0.195490, accuracy: 92.83%\n",
            "step: 1296 | loss: 0.195235, accuracy: 92.84%\n",
            "step: 1297 | loss: 0.195691, accuracy: 92.83%\n",
            "step: 1298 | loss: 0.195558, accuracy: 92.82%\n",
            "step: 1299 | loss: 0.195161, accuracy: 92.84%\n",
            "step: 1300 | loss: 0.195202, accuracy: 92.82%\n",
            "step: 1301 | loss: 0.195576, accuracy: 92.77%\n",
            "step: 1302 | loss: 0.195239, accuracy: 92.80%\n",
            "step: 1303 | loss: 0.195388, accuracy: 92.79%\n",
            "step: 1304 | loss: 0.194978, accuracy: 92.81%\n",
            "step: 1305 | loss: 0.194344, accuracy: 92.84%\n",
            "step: 1306 | loss: 0.194424, accuracy: 92.84%\n",
            "step: 1307 | loss: 0.194323, accuracy: 92.85%\n",
            "step: 1308 | loss: 0.194109, accuracy: 92.86%\n",
            "step: 1309 | loss: 0.195203, accuracy: 92.83%\n",
            "step: 1310 | loss: 0.195177, accuracy: 92.82%\n",
            "step: 1311 | loss: 0.194693, accuracy: 92.85%\n",
            "step: 1312 | loss: 0.194327, accuracy: 92.85%\n",
            "step: 1313 | loss: 0.194219, accuracy: 92.84%\n",
            "step: 1314 | loss: 0.193985, accuracy: 92.85%\n",
            "step: 1315 | loss: 0.194540, accuracy: 92.84%\n",
            "step: 1316 | loss: 0.194410, accuracy: 92.83%\n",
            "step: 1317 | loss: 0.194662, accuracy: 92.81%\n",
            "step: 1318 | loss: 0.194414, accuracy: 92.81%\n",
            "step: 1319 | loss: 0.195607, accuracy: 92.79%\n",
            "step: 1320 | loss: 0.195217, accuracy: 92.81%\n",
            "step: 1321 | loss: 0.197176, accuracy: 92.77%\n",
            "step: 1322 | loss: 0.197318, accuracy: 92.75%\n",
            "step: 1323 | loss: 0.196837, accuracy: 92.77%\n",
            "step: 1324 | loss: 0.196457, accuracy: 92.78%\n",
            "step: 1325 | loss: 0.196199, accuracy: 92.78%\n",
            "step: 1326 | loss: 0.196115, accuracy: 92.79%\n",
            "step: 1327 | loss: 0.196430, accuracy: 92.80%\n",
            "step: 1328 | loss: 0.196285, accuracy: 92.80%\n",
            "step: 1329 | loss: 0.196048, accuracy: 92.83%\n",
            "step: 1330 | loss: 0.196016, accuracy: 92.80%\n",
            "step: 1331 | loss: 0.195502, accuracy: 92.82%\n",
            "step: 1332 | loss: 0.195288, accuracy: 92.85%\n",
            "step: 1333 | loss: 0.195422, accuracy: 92.82%\n",
            "step: 1334 | loss: 0.195167, accuracy: 92.83%\n",
            "step: 1335 | loss: 0.195666, accuracy: 92.82%\n",
            "step: 1336 | loss: 0.195868, accuracy: 92.81%\n",
            "step: 1337 | loss: 0.196647, accuracy: 92.77%\n",
            "step: 1338 | loss: 0.196382, accuracy: 92.78%\n",
            "step: 1339 | loss: 0.195931, accuracy: 92.80%\n",
            "step: 1340 | loss: 0.196013, accuracy: 92.79%\n",
            "step: 1341 | loss: 0.195789, accuracy: 92.80%\n",
            "step: 1342 | loss: 0.196279, accuracy: 92.78%\n",
            "step: 1343 | loss: 0.196384, accuracy: 92.77%\n",
            "step: 1344 | loss: 0.195900, accuracy: 92.79%\n",
            "step: 1345 | loss: 0.195696, accuracy: 92.80%\n",
            "step: 1346 | loss: 0.195807, accuracy: 92.79%\n",
            "step: 1347 | loss: 0.196136, accuracy: 92.77%\n",
            "step: 1348 | loss: 0.195712, accuracy: 92.79%\n",
            "step: 1349 | loss: 0.195585, accuracy: 92.79%\n",
            "step: 1350 | loss: 0.195755, accuracy: 92.76%\n",
            "step: 1351 | loss: 0.196406, accuracy: 92.71%\n",
            "step: 1352 | loss: 0.195982, accuracy: 92.73%\n",
            "step: 1353 | loss: 0.197683, accuracy: 92.71%\n",
            "step: 1354 | loss: 0.197804, accuracy: 92.70%\n",
            "step: 1355 | loss: 0.197335, accuracy: 92.72%\n",
            "step: 1356 | loss: 0.196974, accuracy: 92.72%\n",
            "step: 1357 | loss: 0.196620, accuracy: 92.73%\n",
            "step: 1358 | loss: 0.198133, accuracy: 92.70%\n",
            "step: 1359 | loss: 0.198693, accuracy: 92.67%\n",
            "step: 1360 | loss: 0.198518, accuracy: 92.67%\n",
            "step: 1361 | loss: 0.198761, accuracy: 92.67%\n",
            "step: 1362 | loss: 0.198876, accuracy: 92.68%\n",
            "step: 1363 | loss: 0.199269, accuracy: 92.66%\n",
            "step: 1364 | loss: 0.199033, accuracy: 92.68%\n",
            "step: 1365 | loss: 0.199232, accuracy: 92.68%\n",
            "step: 1366 | loss: 0.199041, accuracy: 92.69%\n",
            "step: 1367 | loss: 0.199397, accuracy: 92.67%\n",
            "step: 1368 | loss: 0.199311, accuracy: 92.66%\n",
            "step: 1369 | loss: 0.199233, accuracy: 92.66%\n",
            "step: 1370 | loss: 0.198997, accuracy: 92.68%\n",
            "step: 1371 | loss: 0.198823, accuracy: 92.67%\n",
            "step: 1372 | loss: 0.198678, accuracy: 92.67%\n",
            "step: 1373 | loss: 0.198236, accuracy: 92.69%\n",
            "step: 1374 | loss: 0.197959, accuracy: 92.70%\n",
            "step: 1375 | loss: 0.198317, accuracy: 92.68%\n",
            "step: 1376 | loss: 0.198319, accuracy: 92.69%\n",
            "step: 1377 | loss: 0.198516, accuracy: 92.68%\n",
            "step: 1378 | loss: 0.198646, accuracy: 92.67%\n",
            "step: 1379 | loss: 0.198441, accuracy: 92.68%\n",
            "step: 1380 | loss: 0.198022, accuracy: 92.70%\n",
            "step: 1381 | loss: 0.198770, accuracy: 92.66%\n",
            "step: 1382 | loss: 0.198641, accuracy: 92.66%\n",
            "step: 1383 | loss: 0.198609, accuracy: 92.65%\n",
            "step: 1384 | loss: 0.198802, accuracy: 92.62%\n",
            "step: 1385 | loss: 0.198392, accuracy: 92.64%\n",
            "step: 1386 | loss: 0.198308, accuracy: 92.64%\n",
            "step: 1387 | loss: 0.198708, accuracy: 92.61%\n",
            "step: 1388 | loss: 0.198562, accuracy: 92.63%\n",
            "step: 1389 | loss: 0.198122, accuracy: 92.65%\n",
            "step: 1390 | loss: 0.197922, accuracy: 92.64%\n",
            "step: 1391 | loss: 0.198362, accuracy: 92.63%\n",
            "step: 1392 | loss: 0.197972, accuracy: 92.65%\n",
            "step: 1393 | loss: 0.197687, accuracy: 92.67%\n",
            "step: 1394 | loss: 0.197544, accuracy: 92.68%\n",
            "step: 1395 | loss: 0.197981, accuracy: 92.67%\n",
            "step: 1396 | loss: 0.198049, accuracy: 92.66%\n",
            "step: 1397 | loss: 0.198484, accuracy: 92.62%\n",
            "step: 1398 | loss: 0.198447, accuracy: 92.61%\n",
            "step: 1399 | loss: 0.198479, accuracy: 92.61%\n",
            "step: 1400 | loss: 0.198333, accuracy: 92.61%\n",
            "step: 1401 | loss: 0.198057, accuracy: 92.63%\n",
            "step: 1402 | loss: 0.199017, accuracy: 92.60%\n",
            "step: 1403 | loss: 0.198760, accuracy: 92.61%\n",
            "step: 1404 | loss: 0.198414, accuracy: 92.62%\n",
            "step: 1405 | loss: 0.199162, accuracy: 92.59%\n",
            "step: 1406 | loss: 0.199025, accuracy: 92.60%\n",
            "step: 1407 | loss: 0.198637, accuracy: 92.62%\n",
            "step: 1408 | loss: 0.198675, accuracy: 92.60%\n",
            "step: 1409 | loss: 0.198286, accuracy: 92.62%\n",
            "step: 1410 | loss: 0.198707, accuracy: 92.61%\n",
            "step: 1411 | loss: 0.198383, accuracy: 92.63%\n",
            "step: 1412 | loss: 0.198661, accuracy: 92.63%\n",
            "step: 1413 | loss: 0.198582, accuracy: 92.64%\n",
            "step: 1414 | loss: 0.198221, accuracy: 92.66%\n",
            "step: 1415 | loss: 0.198047, accuracy: 92.66%\n",
            "step: 1416 | loss: 0.198231, accuracy: 92.63%\n",
            "step: 1417 | loss: 0.198562, accuracy: 92.63%\n",
            "step: 1418 | loss: 0.198714, accuracy: 92.62%\n",
            "step: 1419 | loss: 0.198418, accuracy: 92.64%\n",
            "step: 1420 | loss: 0.198444, accuracy: 92.63%\n",
            "step: 1421 | loss: 0.198775, accuracy: 92.60%\n",
            "step: 1422 | loss: 0.198827, accuracy: 92.59%\n",
            "step: 1423 | loss: 0.198655, accuracy: 92.60%\n",
            "step: 1424 | loss: 0.198781, accuracy: 92.59%\n",
            "step: 1425 | loss: 0.199041, accuracy: 92.58%\n",
            "step: 1426 | loss: 0.198651, accuracy: 92.59%\n",
            "step: 1427 | loss: 0.198398, accuracy: 92.61%\n",
            "step: 1428 | loss: 0.198364, accuracy: 92.62%\n",
            "step: 1429 | loss: 0.198264, accuracy: 92.62%\n",
            "step: 1430 | loss: 0.198133, accuracy: 92.63%\n",
            "step: 1431 | loss: 0.198112, accuracy: 92.62%\n",
            "step: 1432 | loss: 0.198298, accuracy: 92.63%\n",
            "step: 1433 | loss: 0.198094, accuracy: 92.64%\n",
            "step: 1434 | loss: 0.197854, accuracy: 92.65%\n",
            "step: 1435 | loss: 0.197487, accuracy: 92.67%\n",
            "step: 1436 | loss: 0.197445, accuracy: 92.67%\n",
            "step: 1437 | loss: 0.197149, accuracy: 92.69%\n",
            "step: 1438 | loss: 0.197381, accuracy: 92.66%\n",
            "step: 1439 | loss: 0.197901, accuracy: 92.65%\n",
            "step: 1440 | loss: 0.198053, accuracy: 92.65%\n",
            "step: 1441 | loss: 0.198123, accuracy: 92.64%\n",
            "step: 1442 | loss: 0.197826, accuracy: 92.66%\n",
            "step: 1443 | loss: 0.197621, accuracy: 92.66%\n",
            "step: 1444 | loss: 0.197383, accuracy: 92.67%\n",
            "step: 1445 | loss: 0.197186, accuracy: 92.67%\n",
            "step: 1446 | loss: 0.196772, accuracy: 92.69%\n",
            "step: 1447 | loss: 0.196639, accuracy: 92.70%\n",
            "step: 1448 | loss: 0.196618, accuracy: 92.69%\n",
            "step: 1449 | loss: 0.196935, accuracy: 92.67%\n",
            "step: 1450 | loss: 0.197097, accuracy: 92.67%\n",
            "step: 1451 | loss: 0.197081, accuracy: 92.67%\n",
            "Traceback (most recent call last):\n",
            "  File \"train_demo.py\", line 253, in <module>\n",
            "    main()\n",
            "  File \"train_demo.py\", line 242, in main\n",
            "    learning_rate=opt.lr, use_sgd_for_bert=opt.use_sgd_for_bert, grad_iter=opt.grad_iter)\n",
            "  File \"/content/FewRel/fewshot_re_kit/framework.py\", line 219, in train\n",
            "    right = model.accuracy(pred, label)\n",
            "  File \"/content/FewRel/fewshot_re_kit/framework.py\", line 58, in accuracy\n",
            "    return torch.mean((pred.view(-1) == label.view(-1)).type(torch.FloatTensor))\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}